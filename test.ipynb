{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "# import VAE as VAE\n",
    "import VAE.DataLoader as DataLoader\n",
    "import VAE.Encoder as Encoder\n",
    "import VAE.Decoder as Decoder\n",
    "import VAE.ConvSeq2Seq as ConvSeq2Seq\n",
    "import VAE.Discriminator as Discriminator\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'VAE.DataLoader' from '/home/wichen/repos/CS7643_DL/Project/VAE/DataLoader.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(Encoder)\n",
    "reload(Decoder)\n",
    "reload(ConvSeq2Seq)\n",
    "reload(Discriminator)\n",
    "reload(DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check device availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"You are using device: %s\" % device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading subject 1, action walking, subaction 1\n",
      "Reading subject 1, action walking, subaction 2\n",
      "Reading subject 1, action eating, subaction 1\n",
      "Reading subject 1, action eating, subaction 2\n",
      "Reading subject 1, action smoking, subaction 1\n",
      "Reading subject 1, action smoking, subaction 2\n",
      "Reading subject 1, action discussion, subaction 1\n",
      "Reading subject 1, action discussion, subaction 2\n",
      "Reading subject 1, action directions, subaction 1\n",
      "Reading subject 1, action directions, subaction 2\n",
      "Reading subject 1, action greeting, subaction 1\n",
      "Reading subject 1, action greeting, subaction 2\n",
      "Reading subject 1, action phoning, subaction 1\n",
      "Reading subject 1, action phoning, subaction 2\n",
      "Reading subject 1, action posing, subaction 1\n",
      "Reading subject 1, action posing, subaction 2\n",
      "Reading subject 1, action purchases, subaction 1\n",
      "Reading subject 1, action purchases, subaction 2\n",
      "Reading subject 1, action sitting, subaction 1\n",
      "Reading subject 1, action sitting, subaction 2\n",
      "Reading subject 1, action sittingdown, subaction 1\n",
      "Reading subject 1, action sittingdown, subaction 2\n",
      "Reading subject 1, action takingphoto, subaction 1\n",
      "Reading subject 1, action takingphoto, subaction 2\n",
      "Reading subject 1, action waiting, subaction 1\n",
      "Reading subject 1, action waiting, subaction 2\n",
      "Reading subject 1, action walkingdog, subaction 1\n",
      "Reading subject 1, action walkingdog, subaction 2\n",
      "Reading subject 1, action walkingtogether, subaction 1\n",
      "Reading subject 1, action walkingtogether, subaction 2\n",
      "Reading subject 6, action walking, subaction 1\n",
      "Reading subject 6, action walking, subaction 2\n",
      "Reading subject 6, action eating, subaction 1\n",
      "Reading subject 6, action eating, subaction 2\n",
      "Reading subject 6, action smoking, subaction 1\n",
      "Reading subject 6, action smoking, subaction 2\n",
      "Reading subject 6, action discussion, subaction 1\n",
      "Reading subject 6, action discussion, subaction 2\n",
      "Reading subject 6, action directions, subaction 1\n",
      "Reading subject 6, action directions, subaction 2\n",
      "Reading subject 6, action greeting, subaction 1\n",
      "Reading subject 6, action greeting, subaction 2\n",
      "Reading subject 6, action phoning, subaction 1\n",
      "Reading subject 6, action phoning, subaction 2\n",
      "Reading subject 6, action posing, subaction 1\n",
      "Reading subject 6, action posing, subaction 2\n",
      "Reading subject 6, action purchases, subaction 1\n",
      "Reading subject 6, action purchases, subaction 2\n",
      "Reading subject 6, action sitting, subaction 1\n",
      "Reading subject 6, action sitting, subaction 2\n",
      "Reading subject 6, action sittingdown, subaction 1\n",
      "Reading subject 6, action sittingdown, subaction 2\n",
      "Reading subject 6, action takingphoto, subaction 1\n",
      "Reading subject 6, action takingphoto, subaction 2\n",
      "Reading subject 6, action waiting, subaction 1\n",
      "Reading subject 6, action waiting, subaction 2\n",
      "Reading subject 6, action walkingdog, subaction 1\n",
      "Reading subject 6, action walkingdog, subaction 2\n",
      "Reading subject 6, action walkingtogether, subaction 1\n",
      "Reading subject 6, action walkingtogether, subaction 2\n",
      "Reading subject 7, action walking, subaction 1\n",
      "Reading subject 7, action walking, subaction 2\n",
      "Reading subject 7, action eating, subaction 1\n",
      "Reading subject 7, action eating, subaction 2\n",
      "Reading subject 7, action smoking, subaction 1\n",
      "Reading subject 7, action smoking, subaction 2\n",
      "Reading subject 7, action discussion, subaction 1\n",
      "Reading subject 7, action discussion, subaction 2\n",
      "Reading subject 7, action directions, subaction 1\n",
      "Reading subject 7, action directions, subaction 2\n",
      "Reading subject 7, action greeting, subaction 1\n",
      "Reading subject 7, action greeting, subaction 2\n",
      "Reading subject 7, action phoning, subaction 1\n",
      "Reading subject 7, action phoning, subaction 2\n",
      "Reading subject 7, action posing, subaction 1\n",
      "Reading subject 7, action posing, subaction 2\n",
      "Reading subject 7, action purchases, subaction 1\n",
      "Reading subject 7, action purchases, subaction 2\n",
      "Reading subject 7, action sitting, subaction 1\n",
      "Reading subject 7, action sitting, subaction 2\n",
      "Reading subject 7, action sittingdown, subaction 1\n",
      "Reading subject 7, action sittingdown, subaction 2\n",
      "Reading subject 7, action takingphoto, subaction 1\n",
      "Reading subject 7, action takingphoto, subaction 2\n",
      "Reading subject 7, action waiting, subaction 1\n",
      "Reading subject 7, action waiting, subaction 2\n",
      "Reading subject 7, action walkingdog, subaction 1\n",
      "Reading subject 7, action walkingdog, subaction 2\n",
      "Reading subject 7, action walkingtogether, subaction 1\n",
      "Reading subject 7, action walkingtogether, subaction 2\n",
      "Reading subject 8, action walking, subaction 1\n",
      "Reading subject 8, action walking, subaction 2\n",
      "Reading subject 8, action eating, subaction 1\n",
      "Reading subject 8, action eating, subaction 2\n",
      "Reading subject 8, action smoking, subaction 1\n",
      "Reading subject 8, action smoking, subaction 2\n",
      "Reading subject 8, action discussion, subaction 1\n",
      "Reading subject 8, action discussion, subaction 2\n",
      "Reading subject 8, action directions, subaction 1\n",
      "Reading subject 8, action directions, subaction 2\n",
      "Reading subject 8, action greeting, subaction 1\n",
      "Reading subject 8, action greeting, subaction 2\n",
      "Reading subject 8, action phoning, subaction 1\n",
      "Reading subject 8, action phoning, subaction 2\n",
      "Reading subject 8, action posing, subaction 1\n",
      "Reading subject 8, action posing, subaction 2\n",
      "Reading subject 8, action purchases, subaction 1\n",
      "Reading subject 8, action purchases, subaction 2\n",
      "Reading subject 8, action sitting, subaction 1\n",
      "Reading subject 8, action sitting, subaction 2\n",
      "Reading subject 8, action sittingdown, subaction 1\n",
      "Reading subject 8, action sittingdown, subaction 2\n",
      "Reading subject 8, action takingphoto, subaction 1\n",
      "Reading subject 8, action takingphoto, subaction 2\n",
      "Reading subject 8, action waiting, subaction 1\n",
      "Reading subject 8, action waiting, subaction 2\n",
      "Reading subject 8, action walkingdog, subaction 1\n",
      "Reading subject 8, action walkingdog, subaction 2\n",
      "Reading subject 8, action walkingtogether, subaction 1\n",
      "Reading subject 8, action walkingtogether, subaction 2\n",
      "Reading subject 9, action walking, subaction 1\n",
      "Reading subject 9, action walking, subaction 2\n",
      "Reading subject 9, action eating, subaction 1\n",
      "Reading subject 9, action eating, subaction 2\n",
      "Reading subject 9, action smoking, subaction 1\n",
      "Reading subject 9, action smoking, subaction 2\n",
      "Reading subject 9, action discussion, subaction 1\n",
      "Reading subject 9, action discussion, subaction 2\n",
      "Reading subject 9, action directions, subaction 1\n",
      "Reading subject 9, action directions, subaction 2\n",
      "Reading subject 9, action greeting, subaction 1\n",
      "Reading subject 9, action greeting, subaction 2\n",
      "Reading subject 9, action phoning, subaction 1\n",
      "Reading subject 9, action phoning, subaction 2\n",
      "Reading subject 9, action posing, subaction 1\n",
      "Reading subject 9, action posing, subaction 2\n",
      "Reading subject 9, action purchases, subaction 1\n",
      "Reading subject 9, action purchases, subaction 2\n",
      "Reading subject 9, action sitting, subaction 1\n",
      "Reading subject 9, action sitting, subaction 2\n",
      "Reading subject 9, action sittingdown, subaction 1\n",
      "Reading subject 9, action sittingdown, subaction 2\n",
      "Reading subject 9, action takingphoto, subaction 1\n",
      "Reading subject 9, action takingphoto, subaction 2\n",
      "Reading subject 9, action waiting, subaction 1\n",
      "Reading subject 9, action waiting, subaction 2\n",
      "Reading subject 9, action walkingdog, subaction 1\n",
      "Reading subject 9, action walkingdog, subaction 2\n",
      "Reading subject 9, action walkingtogether, subaction 1\n",
      "Reading subject 9, action walkingtogether, subaction 2\n",
      "Reading subject 11, action walking, subaction 1\n",
      "Reading subject 11, action walking, subaction 2\n",
      "Reading subject 11, action eating, subaction 1\n",
      "Reading subject 11, action eating, subaction 2\n",
      "Reading subject 11, action smoking, subaction 1\n",
      "Reading subject 11, action smoking, subaction 2\n",
      "Reading subject 11, action discussion, subaction 1\n",
      "Reading subject 11, action discussion, subaction 2\n",
      "Reading subject 11, action directions, subaction 1\n",
      "Reading subject 11, action directions, subaction 2\n",
      "Reading subject 11, action greeting, subaction 1\n",
      "Reading subject 11, action greeting, subaction 2\n",
      "Reading subject 11, action phoning, subaction 1\n",
      "Reading subject 11, action phoning, subaction 2\n",
      "Reading subject 11, action posing, subaction 1\n",
      "Reading subject 11, action posing, subaction 2\n",
      "Reading subject 11, action purchases, subaction 1\n",
      "Reading subject 11, action purchases, subaction 2\n",
      "Reading subject 11, action sitting, subaction 1\n",
      "Reading subject 11, action sitting, subaction 2\n",
      "Reading subject 11, action sittingdown, subaction 1\n",
      "Reading subject 11, action sittingdown, subaction 2\n",
      "Reading subject 11, action takingphoto, subaction 1\n",
      "Reading subject 11, action takingphoto, subaction 2\n",
      "Reading subject 11, action waiting, subaction 1\n",
      "Reading subject 11, action waiting, subaction 2\n",
      "Reading subject 11, action walkingdog, subaction 1\n",
      "Reading subject 11, action walkingdog, subaction 2\n",
      "Reading subject 11, action walkingtogether, subaction 1\n",
      "Reading subject 11, action walkingtogether, subaction 2\n",
      "Reading subject 5, action walking, subaction 1\n",
      "Reading subject 5, action walking, subaction 2\n",
      "Reading subject 5, action eating, subaction 1\n",
      "Reading subject 5, action eating, subaction 2\n",
      "Reading subject 5, action smoking, subaction 1\n",
      "Reading subject 5, action smoking, subaction 2\n",
      "Reading subject 5, action discussion, subaction 1\n",
      "Reading subject 5, action discussion, subaction 2\n",
      "Reading subject 5, action directions, subaction 1\n",
      "Reading subject 5, action directions, subaction 2\n",
      "Reading subject 5, action greeting, subaction 1\n",
      "Reading subject 5, action greeting, subaction 2\n",
      "Reading subject 5, action phoning, subaction 1\n",
      "Reading subject 5, action phoning, subaction 2\n",
      "Reading subject 5, action posing, subaction 1\n",
      "Reading subject 5, action posing, subaction 2\n",
      "Reading subject 5, action purchases, subaction 1\n",
      "Reading subject 5, action purchases, subaction 2\n",
      "Reading subject 5, action sitting, subaction 1\n",
      "Reading subject 5, action sitting, subaction 2\n",
      "Reading subject 5, action sittingdown, subaction 1\n",
      "Reading subject 5, action sittingdown, subaction 2\n",
      "Reading subject 5, action takingphoto, subaction 1\n",
      "Reading subject 5, action takingphoto, subaction 2\n",
      "Reading subject 5, action waiting, subaction 1\n",
      "Reading subject 5, action waiting, subaction 2\n",
      "Reading subject 5, action walkingdog, subaction 1\n",
      "Reading subject 5, action walkingdog, subaction 2\n",
      "Reading subject 5, action walkingtogether, subaction 1\n",
      "Reading subject 5, action walkingtogether, subaction 2\n",
      "done reading data.\n"
     ]
    }
   ],
   "source": [
    "dloader = DataLoader.DataLoader(50, 25, './data/h3.6m/dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before construct network: 0 bytes => 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "print('Memory usage before construct network: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "                                                   torch.cuda.memory_allocated(device=device)/1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after construct network: 1566820352 bytes => 1.566820352 GB\n"
     ]
    }
   ],
   "source": [
    "lt_encoder = Encoder.Encoder(16,enc_shape=[None, 49, 54, 1], enc_dim_desc={ 'hidden_num': 512,'class_num': 15 })\n",
    "\n",
    "st_encoder = Encoder.Encoder(16,enc_shape=[None, 20, 54, 1], enc_dim_desc={ 'hidden_num': 512})\n",
    "decoder = Decoder.Decoder(st_encoder)\n",
    "\n",
    "generator = ConvSeq2Seq.ConvSeq2Seq(lt_encoder, decoder, window_length=20, device=device)\n",
    "\n",
    "d_encoder = Encoder.Encoder(32, enc_shape=[None, 75,54,1], enc_dim_desc={ 'hidden_num': 512})\n",
    "discriminator = Discriminator.Discriminator(32, d_encoder).to(device)\n",
    "\n",
    "print('Memory usage after construct network: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "                                                   torch.cuda.memory_allocated(device=device)/1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_criterion = nn.MSELoss()\n",
    "D_criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.SGD(discriminator.parameters(), lr=0.0002)\n",
    "optimizerG = optim.SGD(generator.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage when training begins: 3133393408 bytes => 3.133393408 GB\n",
      "encoder_data size: 169344 bytes => 0.000169344 GB\n",
      "discriminator_data size: 259200 bytes => 0.0002592 GB\n",
      "yhat size: 960 bytes => 9.6e-07 GB\n",
      "expected_seq size: 86400 bytes => 8.64e-05 GB\n",
      "Memory usage after discriminator zero_grad: 3133393408 bytes => 3.133393408 GB\n",
      "Memory usage after d_logits_real forward: 3522435584 bytes => 3.522435584 GB\n",
      "Memory usage after d_loss_real: 3522436096 bytes => 3.522436096 GB\n",
      "Memory usage after d_loss_real backward: 3133393408 bytes => 3.133393408 GB\n",
      "Memory usage after generator forward: 4569011200 bytes => 4.5690112 GB\n",
      "Memory usage after d_logits_fake forward: 4957666816 bytes => 4.957666816 GB\n",
      "Memory usage after d_loss_fake : 4957667328 bytes => 4.957667328 GB\n",
      "Memory usage after d_loss_fake backward: 3133393408 bytes => 3.133393408 GB\n",
      "Memory usage after loss_discriminator summed: 3133393408 bytes => 3.133393408 GB\n",
      "Memory usage after optimizerD stepped: 3133393408 bytes => 3.133393408 GB\n",
      "Memory usage after generator forward: 4569011200 bytes => 4.5690112 GB\n",
      "Memory usage after ReconstructError : 4569011200 bytes => 4.5690112 GB\n",
      "Memory usage after ReconstructError backward: 3133393408 bytes => 3.133393408 GB\n",
      "Memory usage after discriminator foward : 3522435584 bytes => 3.522435584 GB\n",
      "Memory usage after g_loss : 3522436608 bytes => 3.522436608 GB\n",
      "Memory usage after loss_generator summed : 3522437120 bytes => 3.52243712 GB\n",
      "Memory usage after optimizerG stepped : 3522437120 bytes => 3.52243712 GB\n",
      "[0/10]\tLoss_D: 1.3863\tLoss_G: 0.8110\n",
      "Memory usage when training begins: 3522437120 bytes => 3.52243712 GB\n",
      "encoder_data size: 169344 bytes => 0.000169344 GB\n",
      "discriminator_data size: 259200 bytes => 0.0002592 GB\n",
      "yhat size: 960 bytes => 9.6e-07 GB\n",
      "expected_seq size: 86400 bytes => 8.64e-05 GB\n",
      "Memory usage after discriminator zero_grad: 3522437120 bytes => 3.52243712 GB\n",
      "Memory usage after d_logits_real forward: 3911092736 bytes => 3.911092736 GB\n",
      "Memory usage after d_loss_real: 3911093248 bytes => 3.911093248 GB\n",
      "Memory usage after d_loss_real backward: 3522437120 bytes => 3.52243712 GB\n",
      "Memory usage after generator forward: 4956566528 bytes => 4.956566528 GB\n",
      "Memory usage after d_logits_fake forward: 5345222656 bytes => 5.345222656 GB\n",
      "Memory usage after d_loss_fake : 5345223168 bytes => 5.345223168 GB\n",
      "Memory usage after d_loss_fake backward: 3522437632 bytes => 3.522437632 GB\n",
      "Memory usage after loss_discriminator summed: 3522437632 bytes => 3.522437632 GB\n",
      "Memory usage after optimizerD stepped: 3522437632 bytes => 3.522437632 GB\n",
      "Memory usage after generator forward: 4956567040 bytes => 4.95656704 GB\n",
      "Memory usage after ReconstructError : 4956567040 bytes => 4.95656704 GB\n",
      "Memory usage after ReconstructError backward: 3522437632 bytes => 3.522437632 GB\n",
      "Memory usage after discriminator foward : 3911093248 bytes => 3.911093248 GB\n",
      "Memory usage after g_loss : 3911093760 bytes => 3.91109376 GB\n",
      "Memory usage after loss_generator summed : 3522050560 bytes => 3.52205056 GB\n",
      "Memory usage after optimizerG stepped : 3522050560 bytes => 3.52205056 GB\n",
      "[1/10]\tLoss_D: 1.3999\tLoss_G: 0.6169\n",
      "Memory usage when training begins: 3522050560 bytes => 3.52205056 GB\n",
      "encoder_data size: 169344 bytes => 0.000169344 GB\n",
      "discriminator_data size: 259200 bytes => 0.0002592 GB\n",
      "yhat size: 960 bytes => 9.6e-07 GB\n",
      "expected_seq size: 86400 bytes => 8.64e-05 GB\n",
      "Memory usage after discriminator zero_grad: 3522050560 bytes => 3.52205056 GB\n",
      "Memory usage after d_logits_real forward: 3911092736 bytes => 3.911092736 GB\n",
      "Memory usage after d_loss_real: 3911093248 bytes => 3.911093248 GB\n",
      "Memory usage after d_loss_real backward: 3522050560 bytes => 3.52205056 GB\n",
      "Memory usage after generator forward: 4958778368 bytes => 4.958778368 GB\n",
      "Memory usage after d_logits_fake forward: 5347434496 bytes => 5.347434496 GB\n",
      "Memory usage after d_loss_fake : 5347435008 bytes => 5.347435008 GB\n",
      "Memory usage after d_loss_fake backward: 3522051072 bytes => 3.522051072 GB\n",
      "Memory usage after loss_discriminator summed: 3522051072 bytes => 3.522051072 GB\n",
      "Memory usage after optimizerD stepped: 3522051072 bytes => 3.522051072 GB\n",
      "Memory usage after generator forward: 4958778880 bytes => 4.95877888 GB\n",
      "Memory usage after ReconstructError : 4958778880 bytes => 4.95877888 GB\n",
      "Memory usage after ReconstructError backward: 3522051072 bytes => 3.522051072 GB\n",
      "Memory usage after discriminator foward : 3911093248 bytes => 3.911093248 GB\n",
      "Memory usage after g_loss : 3911093760 bytes => 3.91109376 GB\n",
      "Memory usage after loss_generator summed : 3522437120 bytes => 3.52243712 GB\n",
      "Memory usage after optimizerG stepped : 3522437120 bytes => 3.52243712 GB\n",
      "[2/10]\tLoss_D: 1.3773\tLoss_G: 0.6631\n",
      "Memory usage when training begins: 3522437120 bytes => 3.52243712 GB\n",
      "encoder_data size: 169344 bytes => 0.000169344 GB\n",
      "discriminator_data size: 259200 bytes => 0.0002592 GB\n",
      "yhat size: 960 bytes => 9.6e-07 GB\n",
      "expected_seq size: 86400 bytes => 8.64e-05 GB\n",
      "Memory usage after discriminator zero_grad: 3522437120 bytes => 3.52243712 GB\n",
      "Memory usage after d_logits_real forward: 3911092736 bytes => 3.911092736 GB\n",
      "Memory usage after d_loss_real: 3911093248 bytes => 3.911093248 GB\n",
      "Memory usage after d_loss_real backward: 3522437120 bytes => 3.52243712 GB\n",
      "Memory usage after generator forward: 4956566528 bytes => 4.956566528 GB\n",
      "Memory usage after d_logits_fake forward: 5345222656 bytes => 5.345222656 GB\n",
      "Memory usage after d_loss_fake : 5345223168 bytes => 5.345223168 GB\n",
      "Memory usage after d_loss_fake backward: 3522437632 bytes => 3.522437632 GB\n",
      "Memory usage after loss_discriminator summed: 3522437632 bytes => 3.522437632 GB\n",
      "Memory usage after optimizerD stepped: 3522437632 bytes => 3.522437632 GB\n",
      "Memory usage after generator forward: 4956567040 bytes => 4.95656704 GB\n",
      "Memory usage after ReconstructError : 4956567040 bytes => 4.95656704 GB\n",
      "Memory usage after ReconstructError backward: 3522437632 bytes => 3.522437632 GB\n",
      "Memory usage after discriminator foward : 3911093248 bytes => 3.911093248 GB\n",
      "Memory usage after g_loss : 3911093760 bytes => 3.91109376 GB\n",
      "Memory usage after loss_generator summed : 3522050560 bytes => 3.52205056 GB\n",
      "Memory usage after optimizerG stepped : 3522050560 bytes => 3.52205056 GB\n",
      "[3/10]\tLoss_D: 1.3995\tLoss_G: 0.4312\n",
      "Memory usage when training begins: 3522050560 bytes => 3.52205056 GB\n",
      "encoder_data size: 169344 bytes => 0.000169344 GB\n",
      "discriminator_data size: 259200 bytes => 0.0002592 GB\n",
      "yhat size: 960 bytes => 9.6e-07 GB\n",
      "expected_seq size: 86400 bytes => 8.64e-05 GB\n",
      "Memory usage after discriminator zero_grad: 3522050560 bytes => 3.52205056 GB\n",
      "Memory usage after d_logits_real forward: 3911092736 bytes => 3.911092736 GB\n",
      "Memory usage after d_loss_real: 3911093248 bytes => 3.911093248 GB\n",
      "Memory usage after d_loss_real backward: 3522050560 bytes => 3.52205056 GB\n",
      "Memory usage after generator forward: 4958778368 bytes => 4.958778368 GB\n",
      "Memory usage after d_logits_fake forward: 5347434496 bytes => 5.347434496 GB\n",
      "Memory usage after d_loss_fake : 5347435008 bytes => 5.347435008 GB\n",
      "Memory usage after d_loss_fake backward: 3522051072 bytes => 3.522051072 GB\n",
      "Memory usage after loss_discriminator summed: 3522051072 bytes => 3.522051072 GB\n",
      "Memory usage after optimizerD stepped: 3522051072 bytes => 3.522051072 GB\n",
      "Memory usage after generator forward: 4958778880 bytes => 4.95877888 GB\n",
      "Memory usage after ReconstructError : 4958778880 bytes => 4.95877888 GB\n",
      "Memory usage after ReconstructError backward: 3522051072 bytes => 3.522051072 GB\n",
      "Memory usage after discriminator foward : 3911093248 bytes => 3.911093248 GB\n",
      "Memory usage after g_loss : 3911093760 bytes => 3.91109376 GB\n",
      "Memory usage after loss_generator summed : 3522437120 bytes => 3.52243712 GB\n",
      "Memory usage after optimizerG stepped : 3522437120 bytes => 3.52243712 GB\n",
      "[4/10]\tLoss_D: 1.3869\tLoss_G: 0.8181\n",
      "Memory usage when training begins: 3522437120 bytes => 3.52243712 GB\n",
      "encoder_data size: 169344 bytes => 0.000169344 GB\n",
      "discriminator_data size: 259200 bytes => 0.0002592 GB\n",
      "yhat size: 960 bytes => 9.6e-07 GB\n",
      "expected_seq size: 86400 bytes => 8.64e-05 GB\n",
      "Memory usage after discriminator zero_grad: 3522437120 bytes => 3.52243712 GB\n",
      "Memory usage after d_logits_real forward: 3911092736 bytes => 3.911092736 GB\n",
      "Memory usage after d_loss_real: 3911093248 bytes => 3.911093248 GB\n",
      "Memory usage after d_loss_real backward: 3522437120 bytes => 3.52243712 GB\n",
      "Memory usage after generator forward: 4956566528 bytes => 4.956566528 GB\n",
      "Memory usage after d_logits_fake forward: 5345222656 bytes => 5.345222656 GB\n",
      "Memory usage after d_loss_fake : 5345223168 bytes => 5.345223168 GB\n",
      "Memory usage after d_loss_fake backward: 3522437632 bytes => 3.522437632 GB\n",
      "Memory usage after loss_discriminator summed: 3522437632 bytes => 3.522437632 GB\n",
      "Memory usage after optimizerD stepped: 3522437632 bytes => 3.522437632 GB\n",
      "Memory usage after generator forward: 4956567040 bytes => 4.95656704 GB\n",
      "Memory usage after ReconstructError : 4956567040 bytes => 4.95656704 GB\n",
      "Memory usage after ReconstructError backward: 3522437632 bytes => 3.522437632 GB\n",
      "Memory usage after discriminator foward : 3911093248 bytes => 3.911093248 GB\n",
      "Memory usage after g_loss : 3911093760 bytes => 3.91109376 GB\n",
      "Memory usage after loss_generator summed : 3522050560 bytes => 3.52205056 GB\n",
      "Memory usage after optimizerG stepped : 3522050560 bytes => 3.52205056 GB\n",
      "[5/10]\tLoss_D: 1.3742\tLoss_G: 0.4656\n",
      "Memory usage when training begins: 3522050560 bytes => 3.52205056 GB\n",
      "encoder_data size: 169344 bytes => 0.000169344 GB\n",
      "discriminator_data size: 259200 bytes => 0.0002592 GB\n",
      "yhat size: 960 bytes => 9.6e-07 GB\n",
      "expected_seq size: 86400 bytes => 8.64e-05 GB\n",
      "Memory usage after discriminator zero_grad: 3522050560 bytes => 3.52205056 GB\n",
      "Memory usage after d_logits_real forward: 3911092736 bytes => 3.911092736 GB\n",
      "Memory usage after d_loss_real: 3911093248 bytes => 3.911093248 GB\n",
      "Memory usage after d_loss_real backward: 3522050560 bytes => 3.52205056 GB\n",
      "Memory usage after generator forward: 4958778368 bytes => 4.958778368 GB\n",
      "Memory usage after d_logits_fake forward: 5347434496 bytes => 5.347434496 GB\n",
      "Memory usage after d_loss_fake : 5347435008 bytes => 5.347435008 GB\n",
      "Memory usage after d_loss_fake backward: 3522051072 bytes => 3.522051072 GB\n",
      "Memory usage after loss_discriminator summed: 3522051072 bytes => 3.522051072 GB\n",
      "Memory usage after optimizerD stepped: 3522051072 bytes => 3.522051072 GB\n",
      "Memory usage after generator forward: 4958778880 bytes => 4.95877888 GB\n",
      "Memory usage after ReconstructError : 4958778880 bytes => 4.95877888 GB\n",
      "Memory usage after ReconstructError backward: 3522051072 bytes => 3.522051072 GB\n",
      "Memory usage after discriminator foward : 3911093248 bytes => 3.911093248 GB\n",
      "Memory usage after g_loss : 3911093760 bytes => 3.91109376 GB\n",
      "Memory usage after loss_generator summed : 3522437120 bytes => 3.52243712 GB\n",
      "Memory usage after optimizerG stepped : 3522437120 bytes => 3.52243712 GB\n",
      "[6/10]\tLoss_D: 1.3807\tLoss_G: 0.4585\n",
      "Memory usage when training begins: 3522437120 bytes => 3.52243712 GB\n",
      "encoder_data size: 169344 bytes => 0.000169344 GB\n",
      "discriminator_data size: 259200 bytes => 0.0002592 GB\n",
      "yhat size: 960 bytes => 9.6e-07 GB\n",
      "expected_seq size: 86400 bytes => 8.64e-05 GB\n",
      "Memory usage after discriminator zero_grad: 3522437120 bytes => 3.52243712 GB\n",
      "Memory usage after d_logits_real forward: 3911092736 bytes => 3.911092736 GB\n",
      "Memory usage after d_loss_real: 3911093248 bytes => 3.911093248 GB\n",
      "Memory usage after d_loss_real backward: 3522437120 bytes => 3.52243712 GB\n",
      "Memory usage after generator forward: 4956566528 bytes => 4.956566528 GB\n",
      "Memory usage after d_logits_fake forward: 5345222656 bytes => 5.345222656 GB\n",
      "Memory usage after d_loss_fake : 5345223168 bytes => 5.345223168 GB\n",
      "Memory usage after d_loss_fake backward: 3522437632 bytes => 3.522437632 GB\n",
      "Memory usage after loss_discriminator summed: 3522437632 bytes => 3.522437632 GB\n",
      "Memory usage after optimizerD stepped: 3522437632 bytes => 3.522437632 GB\n",
      "Memory usage after generator forward: 4956567040 bytes => 4.95656704 GB\n",
      "Memory usage after ReconstructError : 4956567040 bytes => 4.95656704 GB\n",
      "Memory usage after ReconstructError backward: 3522437632 bytes => 3.522437632 GB\n",
      "Memory usage after discriminator foward : 3911093248 bytes => 3.911093248 GB\n",
      "Memory usage after g_loss : 3911093760 bytes => 3.91109376 GB\n",
      "Memory usage after loss_generator summed : 3522050560 bytes => 3.52205056 GB\n",
      "Memory usage after optimizerG stepped : 3522050560 bytes => 3.52205056 GB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4724/4186116046.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         print('[%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f'\n\u001b[0;32m---> 93\u001b[0;31m             % (i, 10, loss_discriminator.item(), loss_generator.item()))\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    \n",
    "    print('Memory usage when training begins: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "                                                   torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    \n",
    "    print(\"****************train discriminator*****************\")\n",
    "    encoder_data, discriminator_data, yhat = dloader.get_train_batch(16)\n",
    "\n",
    "    encoder_data = torch.from_numpy(encoder_data).float().to(device)\n",
    "    print(\"encoder_data size: {} bytes => {} GB\".format(encoder_data.element_size() * encoder_data.nelement(),\n",
    "                                                        encoder_data.element_size() * encoder_data.nelement()/1e9))\n",
    "    \n",
    "    discriminator_data = torch.from_numpy(discriminator_data).float().to(device)\n",
    "    print(\"discriminator_data size: {} bytes => {} GB\".format(discriminator_data.element_size() * discriminator_data.nelement(),\n",
    "                                                              discriminator_data.element_size() * discriminator_data.nelement()/1e9))\n",
    "    \n",
    "    yhat = torch.from_numpy(yhat).float().to(device)\n",
    "    print(\"yhat size: {} bytes => {} GB\".format(yhat.element_size() * yhat.nelement(),\n",
    "                                                yhat.element_size() * yhat.nelement()/1e9))\n",
    "    \n",
    "    # expected_seq = discriminator_data[:, 50:, :, :]\n",
    "    # print(\"expected_seq size: {} bytes => {} GB\".format(expected_seq.element_size() * expected_seq.nelement(),\n",
    "    #                                                     expected_seq.element_size() * expected_seq.nelement()/1e9))\n",
    "\n",
    "    discriminator.zero_grad()\n",
    "    print('Memory usage after discriminator zero_grad: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "                                                   torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    \n",
    "    d_logits_real = discriminator.forward(discriminator_data, yhat)\n",
    "    print('Memory usage after d_logits_real forward: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "                                                   torch.cuda.memory_allocated(device=device)/1e9))\n",
    "\n",
    "    d_loss_real = D_criterion(d_logits_real, torch.ones_like(d_logits_real))\n",
    "    print('Memory usage after d_loss_real: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "                                                   torch.cuda.memory_allocated(device=device)/1e9))\n",
    "\n",
    "    d_loss_real.backward()\n",
    "    print('Memory usage after d_loss_real backward: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "                                                   torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    \n",
    "    predicted_seq, predicted_action, generated_sample = generator.forward(encoder_data, discriminator_data)\n",
    "    \n",
    "    print('Memory usage after generator forward: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "                                                   torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    d_logits_fake = discriminator.forward(generated_sample, yhat)\n",
    "    print('Memory usage after d_logits_fake forward: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "                                                   torch.cuda.memory_allocated(device=device)/1e9))\n",
    "\n",
    "    d_loss_fake = D_criterion(d_logits_fake, torch.zeros_like(d_logits_fake))\n",
    "    print('Memory usage after d_loss_fake : {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "                                                   torch.cuda.memory_allocated(device=device)/1e9))\n",
    "\n",
    "    d_loss_fake.backward()\n",
    "    print('Memory usage after d_loss_fake backward: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "                                                   torch.cuda.memory_allocated(device=device)/1e9))\n",
    "\n",
    "    loss_discriminator = d_loss_real + d_loss_fake\n",
    "    print('Memory usage after loss_discriminator summed: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "                                                   torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    optimizerD.step()\n",
    "    print('Memory usage after optimizerD stepped: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "                                                   torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    \n",
    "    print(\"****************train generator*****************\")\n",
    "    encoder_data, discriminator_data, yhat = dloader.get_train_batch(16)\n",
    "\n",
    "    encoder_data = torch.from_numpy(encoder_data).float().to(device)\n",
    "    print(\"encoder_data size: {} bytes => {} GB\".format(encoder_data.element_size() * encoder_data.nelement(),\n",
    "                                                        encoder_data.element_size() * encoder_data.nelement()/1e9))\n",
    "    \n",
    "    discriminator_data = torch.from_numpy(discriminator_data).float().to(device)\n",
    "    print(\"discriminator_data size: {} bytes => {} GB\".format(discriminator_data.element_size() * discriminator_data.nelement(),\n",
    "                                                              discriminator_data.element_size() * discriminator_data.nelement()/1e9))\n",
    "    \n",
    "    yhat = torch.from_numpy(yhat).float().to(device)\n",
    "    print(\"yhat size: {} bytes => {} GB\".format(yhat.element_size() * yhat.nelement(),\n",
    "                                                yhat.element_size() * yhat.nelement()/1e9))\n",
    "    \n",
    "    expected_seq = discriminator_data[:, 50:, :, :]\n",
    "    print(\"expected_seq size: {} bytes => {} GB\".format(expected_seq.element_size() * expected_seq.nelement(),\n",
    "                                                        expected_seq.element_size() * expected_seq.nelement()/1e9))\n",
    "    \n",
    "    generator.zero_grad()\n",
    "    \n",
    "    predicted_seq, predicted_action, generated_sample = generator.forward(encoder_data, discriminator_data)\n",
    "    print('Memory usage after generator forward: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "                                                   torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    ReconstructError = G_criterion(predicted_seq, expected_seq)\n",
    "    print('Memory usage after ReconstructError : {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "                                                   torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    ReconstructError.backward()\n",
    "    print('Memory usage after ReconstructError backward: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "                                                   torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    d_logits_fake = discriminator.forward(generated_sample, yhat)\n",
    "    print('Memory usage after discriminator foward : {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "                                                   torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    g_loss = D_criterion(d_logits_fake, torch.ones_like(d_logits_fake)) * torch.tensor(0.01)\n",
    "    print('Memory usage after g_loss : {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "                                                   torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    # g_loss.backward()\n",
    "    # print('Memory usage after g_loss backward: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "    #                                                torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    loss_generator = ReconstructError + g_loss\n",
    "    print('Memory usage after loss_generator summed : {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "                                                   torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    \n",
    "    optimizerG.step()\n",
    "    print('Memory usage after optimizerG stepped : {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "                                                   torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    \n",
    "    # Output training stats\n",
    "    if i % 1 == 0:\n",
    "        print('[%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f'\n",
    "            % (i, 10, loss_discriminator.item(), loss_generator.item()))\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 2            |        cudaMalloc retries: 3         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    5721 MB |    7508 MB |   17326 MB |   11604 MB |\n",
      "|       from large pool |    5687 MB |    7503 MB |   17212 MB |   11524 MB |\n",
      "|       from small pool |      33 MB |      35 MB |     114 MB |      80 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    5721 MB |    7508 MB |   17326 MB |   11604 MB |\n",
      "|       from large pool |    5687 MB |    7503 MB |   17212 MB |   11524 MB |\n",
      "|       from small pool |      33 MB |      35 MB |     114 MB |      80 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    7450 MB |    7554 MB |    9248 MB |    1798 MB |\n",
      "|       from large pool |    7414 MB |    7542 MB |    9188 MB |    1774 MB |\n",
      "|       from small pool |      36 MB |      36 MB |      60 MB |      24 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    1728 MB |    2044 MB |    7591 MB |    5863 MB |\n",
      "|       from large pool |    1726 MB |    2042 MB |    7462 MB |    5736 MB |\n",
      "|       from small pool |       2 MB |       7 MB |     129 MB |     127 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     749    |     760    |    3579    |    2830    |\n",
      "|       from large pool |     274    |     275    |    1115    |     841    |\n",
      "|       from small pool |     475    |     497    |    2464    |    1989    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     749    |     760    |    3579    |    2830    |\n",
      "|       from large pool |     274    |     275    |    1115    |     841    |\n",
      "|       from small pool |     475    |     497    |    2464    |    1989    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      36    |      74    |      87    |      51    |\n",
      "|       from large pool |      18    |      56    |      57    |      39    |\n",
      "|       from small pool |      18    |      18    |      30    |      12    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      38    |      44    |    1576    |    1538    |\n",
      "|       from large pool |       6    |      13    |     481    |     475    |\n",
      "|       from small pool |      32    |      33    |    1095    |    1063    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_data, discriminator_data, yhat = dloader.get_train_batch(32)\n",
    "encoder_data = torch.FloatTensor(encoder_data).to('cuda')\n",
    "discriminator_data = torch.FloatTensor(discriminator_data).to('cuda')\n",
    "yhat = torch.FloatTensor(yhat).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt_encoder = Encoder.Encoder(3,enc_shape=[None, 49, 54, 1], enc_dim_desc={ 'hidden_num': 512,'class_num': 15 })\n",
    "\n",
    "st_encoder = Encoder.Encoder(3,enc_shape=[None, 20, 54, 1], enc_dim_desc={ 'hidden_num': 512})\n",
    "decoder = Decoder.Decoder(st_encoder)\n",
    "\n",
    "generator = ConvSeq2Seq.ConvSeq2Seq(lt_encoder, decoder, window_length=20, device=device)\n",
    "\n",
    "d_encoder = Encoder.Encoder(32, enc_shape=[None, 75,54,1], enc_dim_desc={ 'hidden_num': 512})\n",
    "discriminator = Discriminator.Discriminator(32, d_encoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_criterion = nn.MSELoss()\n",
    "D_criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_seq, predicted_action, generated_sample = generator.forward(encoder_data, discriminator_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_seq = discriminator_data[:, 50:, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 25, 54, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 25, 54, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "expected_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 75, 54, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generated_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReconstructError = G_criterion(predicted_seq, expected_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_logits_real = discriminator.forward(discriminator_data, yhat)\n",
    "d_logits_fake = discriminator.forward(generated_sample, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_loss_real = torch.mean(D_criterion(d_logits_real, torch.ones_like(d_logits_real)))\n",
    "d_loss_fake = torch.mean(D_criterion(d_logits_fake, torch.zeros_like(d_logits_fake)))\n",
    "g_loss = torch.mean(D_criterion(d_logits_fake, torch.ones_like(d_logits_fake)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_d = d_loss_real + d_loss_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3915, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "\u001b[0;32m/tmp/ipykernel_3145/2427296278.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conv-seq2seq-torch-gpu/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n",
      "\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n",
      "\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n",
      "\u001b[0;32m~/anaconda3/envs/conv-seq2seq-torch-gpu/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n",
      "\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n",
      "\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "loss_d.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6923, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_data, discriminator_data, yhat = dloader.get_train_batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05696892], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_data[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_data, discriminator_data, yhat = dloader.get_train_batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.14145434], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_data[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1738, 54)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dloader.train_set[(1,'walking', 1, 'even')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = VAE.EncoderBase(1,2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint( 16, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 4])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "for x in range(0, 10, 2):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '{0}/S{1}/{2}_{3}.txt'.format('./data/h3.6m/dataset', 1, 'walking', 1)\n",
    "action_sequence = data_utils.readCSVasFloat(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3476, 99)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e9b5e93101e5958f9cd028a9f43ed128e885f27a05c22831a3d71acc3bca11d"
  },
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
