{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "# import VAE as VAE\n",
    "import VAE.DataLoader as DataLoader\n",
    "import VAE.Encoder as Encoder\n",
    "import VAE.Decoder as Decoder\n",
    "import VAE.ConvSeq2Seq as ConvSeq2Seq\n",
    "import VAE.Discriminator as Discriminator\n",
    "import util as util\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'util' from '/home/wichen/repos/CS7643_DL/Project/util.py'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(Encoder)\n",
    "reload(Decoder)\n",
    "reload(ConvSeq2Seq)\n",
    "reload(Discriminator)\n",
    "reload(DataLoader)\n",
    "reload(util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check device availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"You are using device: %s\" % device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading subject 1, action walking, subaction 1\n",
      "Reading subject 1, action walking, subaction 2\n",
      "Reading subject 1, action eating, subaction 1\n",
      "Reading subject 1, action eating, subaction 2\n",
      "Reading subject 1, action smoking, subaction 1\n",
      "Reading subject 1, action smoking, subaction 2\n",
      "Reading subject 1, action discussion, subaction 1\n",
      "Reading subject 1, action discussion, subaction 2\n",
      "Reading subject 1, action directions, subaction 1\n",
      "Reading subject 1, action directions, subaction 2\n",
      "Reading subject 1, action greeting, subaction 1\n",
      "Reading subject 1, action greeting, subaction 2\n",
      "Reading subject 1, action phoning, subaction 1\n",
      "Reading subject 1, action phoning, subaction 2\n",
      "Reading subject 1, action posing, subaction 1\n",
      "Reading subject 1, action posing, subaction 2\n",
      "Reading subject 1, action purchases, subaction 1\n",
      "Reading subject 1, action purchases, subaction 2\n",
      "Reading subject 1, action sitting, subaction 1\n",
      "Reading subject 1, action sitting, subaction 2\n",
      "Reading subject 1, action sittingdown, subaction 1\n",
      "Reading subject 1, action sittingdown, subaction 2\n",
      "Reading subject 1, action takingphoto, subaction 1\n",
      "Reading subject 1, action takingphoto, subaction 2\n",
      "Reading subject 1, action waiting, subaction 1\n",
      "Reading subject 1, action waiting, subaction 2\n",
      "Reading subject 1, action walkingdog, subaction 1\n",
      "Reading subject 1, action walkingdog, subaction 2\n",
      "Reading subject 1, action walkingtogether, subaction 1\n",
      "Reading subject 1, action walkingtogether, subaction 2\n",
      "Reading subject 6, action walking, subaction 1\n",
      "Reading subject 6, action walking, subaction 2\n",
      "Reading subject 6, action eating, subaction 1\n",
      "Reading subject 6, action eating, subaction 2\n",
      "Reading subject 6, action smoking, subaction 1\n",
      "Reading subject 6, action smoking, subaction 2\n",
      "Reading subject 6, action discussion, subaction 1\n",
      "Reading subject 6, action discussion, subaction 2\n",
      "Reading subject 6, action directions, subaction 1\n",
      "Reading subject 6, action directions, subaction 2\n",
      "Reading subject 6, action greeting, subaction 1\n",
      "Reading subject 6, action greeting, subaction 2\n",
      "Reading subject 6, action phoning, subaction 1\n",
      "Reading subject 6, action phoning, subaction 2\n",
      "Reading subject 6, action posing, subaction 1\n",
      "Reading subject 6, action posing, subaction 2\n",
      "Reading subject 6, action purchases, subaction 1\n",
      "Reading subject 6, action purchases, subaction 2\n",
      "Reading subject 6, action sitting, subaction 1\n",
      "Reading subject 6, action sitting, subaction 2\n",
      "Reading subject 6, action sittingdown, subaction 1\n",
      "Reading subject 6, action sittingdown, subaction 2\n",
      "Reading subject 6, action takingphoto, subaction 1\n",
      "Reading subject 6, action takingphoto, subaction 2\n",
      "Reading subject 6, action waiting, subaction 1\n",
      "Reading subject 6, action waiting, subaction 2\n",
      "Reading subject 6, action walkingdog, subaction 1\n",
      "Reading subject 6, action walkingdog, subaction 2\n",
      "Reading subject 6, action walkingtogether, subaction 1\n",
      "Reading subject 6, action walkingtogether, subaction 2\n",
      "Reading subject 7, action walking, subaction 1\n",
      "Reading subject 7, action walking, subaction 2\n",
      "Reading subject 7, action eating, subaction 1\n",
      "Reading subject 7, action eating, subaction 2\n",
      "Reading subject 7, action smoking, subaction 1\n",
      "Reading subject 7, action smoking, subaction 2\n",
      "Reading subject 7, action discussion, subaction 1\n",
      "Reading subject 7, action discussion, subaction 2\n",
      "Reading subject 7, action directions, subaction 1\n",
      "Reading subject 7, action directions, subaction 2\n",
      "Reading subject 7, action greeting, subaction 1\n",
      "Reading subject 7, action greeting, subaction 2\n",
      "Reading subject 7, action phoning, subaction 1\n",
      "Reading subject 7, action phoning, subaction 2\n",
      "Reading subject 7, action posing, subaction 1\n",
      "Reading subject 7, action posing, subaction 2\n",
      "Reading subject 7, action purchases, subaction 1\n",
      "Reading subject 7, action purchases, subaction 2\n",
      "Reading subject 7, action sitting, subaction 1\n",
      "Reading subject 7, action sitting, subaction 2\n",
      "Reading subject 7, action sittingdown, subaction 1\n",
      "Reading subject 7, action sittingdown, subaction 2\n",
      "Reading subject 7, action takingphoto, subaction 1\n",
      "Reading subject 7, action takingphoto, subaction 2\n",
      "Reading subject 7, action waiting, subaction 1\n",
      "Reading subject 7, action waiting, subaction 2\n",
      "Reading subject 7, action walkingdog, subaction 1\n",
      "Reading subject 7, action walkingdog, subaction 2\n",
      "Reading subject 7, action walkingtogether, subaction 1\n",
      "Reading subject 7, action walkingtogether, subaction 2\n",
      "Reading subject 8, action walking, subaction 1\n",
      "Reading subject 8, action walking, subaction 2\n",
      "Reading subject 8, action eating, subaction 1\n",
      "Reading subject 8, action eating, subaction 2\n",
      "Reading subject 8, action smoking, subaction 1\n",
      "Reading subject 8, action smoking, subaction 2\n",
      "Reading subject 8, action discussion, subaction 1\n",
      "Reading subject 8, action discussion, subaction 2\n",
      "Reading subject 8, action directions, subaction 1\n",
      "Reading subject 8, action directions, subaction 2\n",
      "Reading subject 8, action greeting, subaction 1\n",
      "Reading subject 8, action greeting, subaction 2\n",
      "Reading subject 8, action phoning, subaction 1\n",
      "Reading subject 8, action phoning, subaction 2\n",
      "Reading subject 8, action posing, subaction 1\n",
      "Reading subject 8, action posing, subaction 2\n",
      "Reading subject 8, action purchases, subaction 1\n",
      "Reading subject 8, action purchases, subaction 2\n",
      "Reading subject 8, action sitting, subaction 1\n",
      "Reading subject 8, action sitting, subaction 2\n",
      "Reading subject 8, action sittingdown, subaction 1\n",
      "Reading subject 8, action sittingdown, subaction 2\n",
      "Reading subject 8, action takingphoto, subaction 1\n",
      "Reading subject 8, action takingphoto, subaction 2\n",
      "Reading subject 8, action waiting, subaction 1\n",
      "Reading subject 8, action waiting, subaction 2\n",
      "Reading subject 8, action walkingdog, subaction 1\n",
      "Reading subject 8, action walkingdog, subaction 2\n",
      "Reading subject 8, action walkingtogether, subaction 1\n",
      "Reading subject 8, action walkingtogether, subaction 2\n",
      "Reading subject 9, action walking, subaction 1\n",
      "Reading subject 9, action walking, subaction 2\n",
      "Reading subject 9, action eating, subaction 1\n",
      "Reading subject 9, action eating, subaction 2\n",
      "Reading subject 9, action smoking, subaction 1\n",
      "Reading subject 9, action smoking, subaction 2\n",
      "Reading subject 9, action discussion, subaction 1\n",
      "Reading subject 9, action discussion, subaction 2\n",
      "Reading subject 9, action directions, subaction 1\n",
      "Reading subject 9, action directions, subaction 2\n",
      "Reading subject 9, action greeting, subaction 1\n",
      "Reading subject 9, action greeting, subaction 2\n",
      "Reading subject 9, action phoning, subaction 1\n",
      "Reading subject 9, action phoning, subaction 2\n",
      "Reading subject 9, action posing, subaction 1\n",
      "Reading subject 9, action posing, subaction 2\n",
      "Reading subject 9, action purchases, subaction 1\n",
      "Reading subject 9, action purchases, subaction 2\n",
      "Reading subject 9, action sitting, subaction 1\n",
      "Reading subject 9, action sitting, subaction 2\n",
      "Reading subject 9, action sittingdown, subaction 1\n",
      "Reading subject 9, action sittingdown, subaction 2\n",
      "Reading subject 9, action takingphoto, subaction 1\n",
      "Reading subject 9, action takingphoto, subaction 2\n",
      "Reading subject 9, action waiting, subaction 1\n",
      "Reading subject 9, action waiting, subaction 2\n",
      "Reading subject 9, action walkingdog, subaction 1\n",
      "Reading subject 9, action walkingdog, subaction 2\n",
      "Reading subject 9, action walkingtogether, subaction 1\n",
      "Reading subject 9, action walkingtogether, subaction 2\n",
      "Reading subject 11, action walking, subaction 1\n",
      "Reading subject 11, action walking, subaction 2\n",
      "Reading subject 11, action eating, subaction 1\n",
      "Reading subject 11, action eating, subaction 2\n",
      "Reading subject 11, action smoking, subaction 1\n",
      "Reading subject 11, action smoking, subaction 2\n",
      "Reading subject 11, action discussion, subaction 1\n",
      "Reading subject 11, action discussion, subaction 2\n",
      "Reading subject 11, action directions, subaction 1\n",
      "Reading subject 11, action directions, subaction 2\n",
      "Reading subject 11, action greeting, subaction 1\n",
      "Reading subject 11, action greeting, subaction 2\n",
      "Reading subject 11, action phoning, subaction 1\n",
      "Reading subject 11, action phoning, subaction 2\n",
      "Reading subject 11, action posing, subaction 1\n",
      "Reading subject 11, action posing, subaction 2\n",
      "Reading subject 11, action purchases, subaction 1\n",
      "Reading subject 11, action purchases, subaction 2\n",
      "Reading subject 11, action sitting, subaction 1\n",
      "Reading subject 11, action sitting, subaction 2\n",
      "Reading subject 11, action sittingdown, subaction 1\n",
      "Reading subject 11, action sittingdown, subaction 2\n",
      "Reading subject 11, action takingphoto, subaction 1\n",
      "Reading subject 11, action takingphoto, subaction 2\n",
      "Reading subject 11, action waiting, subaction 1\n",
      "Reading subject 11, action waiting, subaction 2\n",
      "Reading subject 11, action walkingdog, subaction 1\n",
      "Reading subject 11, action walkingdog, subaction 2\n",
      "Reading subject 11, action walkingtogether, subaction 1\n",
      "Reading subject 11, action walkingtogether, subaction 2\n",
      "Reading subject 5, action walking, subaction 1\n",
      "Reading subject 5, action walking, subaction 2\n",
      "Reading subject 5, action eating, subaction 1\n",
      "Reading subject 5, action eating, subaction 2\n",
      "Reading subject 5, action smoking, subaction 1\n",
      "Reading subject 5, action smoking, subaction 2\n",
      "Reading subject 5, action discussion, subaction 1\n",
      "Reading subject 5, action discussion, subaction 2\n",
      "Reading subject 5, action directions, subaction 1\n",
      "Reading subject 5, action directions, subaction 2\n",
      "Reading subject 5, action greeting, subaction 1\n",
      "Reading subject 5, action greeting, subaction 2\n",
      "Reading subject 5, action phoning, subaction 1\n",
      "Reading subject 5, action phoning, subaction 2\n",
      "Reading subject 5, action posing, subaction 1\n",
      "Reading subject 5, action posing, subaction 2\n",
      "Reading subject 5, action purchases, subaction 1\n",
      "Reading subject 5, action purchases, subaction 2\n",
      "Reading subject 5, action sitting, subaction 1\n",
      "Reading subject 5, action sitting, subaction 2\n",
      "Reading subject 5, action sittingdown, subaction 1\n",
      "Reading subject 5, action sittingdown, subaction 2\n",
      "Reading subject 5, action takingphoto, subaction 1\n",
      "Reading subject 5, action takingphoto, subaction 2\n",
      "Reading subject 5, action waiting, subaction 1\n",
      "Reading subject 5, action waiting, subaction 2\n",
      "Reading subject 5, action walkingdog, subaction 1\n",
      "Reading subject 5, action walkingdog, subaction 2\n",
      "Reading subject 5, action walkingtogether, subaction 1\n",
      "Reading subject 5, action walkingtogether, subaction 2\n",
      "done reading data.\n"
     ]
    }
   ],
   "source": [
    "dloader = DataLoader.DataLoader(50, 25, './data/h3.6m/dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage before construct network: 0 bytes => 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "print('Memory usage before construct network: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "                                                   torch.cuda.memory_allocated(device=device)/1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after construct network: 67533312 bytes => 0.067533312 GB\n"
     ]
    }
   ],
   "source": [
    "lt_encoder = Encoder.Encoder(16,enc_shape=[None, 49, 54, 1], enc_dim_desc={ 'hidden_num': 512,'class_num': 15 }, stride=(2,2))\n",
    "\n",
    "st_encoder = Encoder.Encoder(16,enc_shape=[None, 20, 54, 1], enc_dim_desc={ 'hidden_num': 512}, stride=(2,2))\n",
    "decoder = Decoder.Decoder(st_encoder)\n",
    "\n",
    "generator = ConvSeq2Seq.ConvSeq2Seq(lt_encoder, decoder, window_length=20, device=device)\n",
    "\n",
    "d_encoder = Encoder.Encoder(32, enc_shape=[None, 75,54,1], enc_dim_desc={ 'hidden_num': 512}, stride=(2,2))\n",
    "discriminator = Discriminator.Discriminator(32, d_encoder).to(device)\n",
    "\n",
    "print('Memory usage after construct network: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "                                                   torch.cuda.memory_allocated(device=device)/1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.conv1.weight 896\n",
      "encoder.batch_norm1.weight 64\n",
      "encoder.batch_norm1.bias 64\n",
      "encoder.conv2.weight 114688\n",
      "encoder.batch_norm2.weight 128\n",
      "encoder.batch_norm2.bias 128\n",
      "encoder.conv3.weight 229376\n",
      "encoder.batch_norm3.weight 128\n",
      "encoder.batch_norm3.bias 128\n",
      "encoder.fc.weight 3305344\n",
      "encoder.fc.bias 527\n",
      "decoder.st_encoder.conv1.weight 896\n",
      "decoder.st_encoder.batch_norm1.weight 64\n",
      "decoder.st_encoder.batch_norm1.bias 64\n",
      "decoder.st_encoder.conv2.weight 114688\n",
      "decoder.st_encoder.batch_norm2.weight 128\n",
      "decoder.st_encoder.batch_norm2.bias 128\n",
      "decoder.st_encoder.conv3.weight 229376\n",
      "decoder.st_encoder.batch_norm3.weight 128\n",
      "decoder.st_encoder.batch_norm3.bias 128\n",
      "decoder.st_encoder.fc.weight 1376256\n",
      "decoder.st_encoder.fc.bias 512\n",
      "decoder.fc1.weight 524288\n",
      "decoder.fc1.bias 512\n",
      "decoder.fc2.weight 27648\n",
      "decoder.fc2.bias 54\n"
     ]
    }
   ],
   "source": [
    "for name, param in generator.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_encoder.conv1.weight 1792\n",
      "d_encoder.batch_norm1.weight 128\n",
      "d_encoder.batch_norm1.bias 128\n",
      "d_encoder.conv2.weight 458752\n",
      "d_encoder.batch_norm2.weight 256\n",
      "d_encoder.batch_norm2.bias 256\n",
      "d_encoder.conv3.weight 917504\n",
      "d_encoder.batch_norm3.weight 256\n",
      "d_encoder.batch_norm3.bias 256\n",
      "d_encoder.fc.weight 9175040\n",
      "d_encoder.fc.bias 512\n",
      "fc1.weight 134912\n",
      "fc1.bias 256\n",
      "fc2.weight 256\n",
      "fc2.bias 1\n"
     ]
    }
   ],
   "source": [
    "for name, param in discriminator.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create loss function for both G and D\n",
    "G_criterion = nn.MSELoss()\n",
    "D_criterion = nn.BCEWithLogitsLoss()\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=5e-5, weight_decay=0.001)\n",
    "optimizerG = optim.Adam(generator.parameters(), lr=5e-5, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations 0 loss_d 1.395955, loss_g 0.345696, lr 0.000050, time 0.855795\n",
      "Iterations 100 loss_d 1.357304, loss_g 0.227643, lr 0.000050, time 61.039670\n",
      "Iterations 200 loss_d 1.232175, loss_g 0.225955, lr 0.000050, time 117.881552\n",
      "Iterations 300 loss_d 1.144946, loss_g 0.230196, lr 0.000050, time 174.676471\n",
      "Iterations 400 loss_d 1.126419, loss_g 0.216229, lr 0.000050, time 230.243264\n",
      "Iterations 500 loss_d 1.056206, loss_g 0.205054, lr 0.000050, time 286.890993\n",
      "Iterations 600 loss_d 1.069987, loss_g 0.209900, lr 0.000050, time 343.419816\n",
      "Iterations 700 loss_d 1.022101, loss_g 0.205566, lr 0.000050, time 400.199652\n",
      "Iterations 800 loss_d 1.013436, loss_g 0.184817, lr 0.000050, time 457.059442\n",
      "Iterations 900 loss_d 0.985256, loss_g 0.206770, lr 0.000050, time 513.888936\n",
      "Iterations 1000 loss_d 0.990646, loss_g 0.209693, lr 0.000050, time 570.699388\n",
      "Iterations 1100 loss_d 0.945466, loss_g 0.213355, lr 0.000050, time 627.635807\n",
      "Iterations 1200 loss_d 0.934238, loss_g 0.197961, lr 0.000050, time 684.181638\n",
      "Iterations 1300 loss_d 0.958711, loss_g 0.187962, lr 0.000050, time 740.835261\n",
      "Iterations 1400 loss_d 0.925639, loss_g 0.200970, lr 0.000050, time 797.665248\n",
      "Iterations 1500 loss_d 0.906917, loss_g 0.199592, lr 0.000050, time 854.419145\n",
      "Iterations 1600 loss_d 0.910366, loss_g 0.188566, lr 0.000050, time 911.023393\n",
      "Iterations 1700 loss_d 0.892341, loss_g 0.194668, lr 0.000050, time 967.867393\n",
      "Iterations 1800 loss_d 0.890179, loss_g 0.189309, lr 0.000050, time 1024.625411\n",
      "Iterations 1900 loss_d 0.856111, loss_g 0.208615, lr 0.000050, time 1081.896464\n",
      "Iterations 2000 loss_d 0.850152, loss_g 0.194887, lr 0.000050, time 1139.044030\n",
      "Iterations 2100 loss_d 0.827760, loss_g 0.195873, lr 0.000050, time 1195.956937\n",
      "Iterations 2200 loss_d 0.866046, loss_g 0.204170, lr 0.000050, time 1252.911431\n",
      "Iterations 2300 loss_d 0.821462, loss_g 0.207710, lr 0.000050, time 1309.633213\n",
      "Iterations 2400 loss_d 0.786174, loss_g 0.202784, lr 0.000050, time 1366.461852\n",
      "Iterations 2500 loss_d 0.780960, loss_g 0.200257, lr 0.000050, time 1423.332986\n",
      "Iterations 2600 loss_d 0.763158, loss_g 0.194881, lr 0.000050, time 1480.144591\n",
      "Iterations 2700 loss_d 0.729657, loss_g 0.200578, lr 0.000050, time 1537.054212\n",
      "Iterations 2800 loss_d 0.724363, loss_g 0.191970, lr 0.000050, time 1594.013503\n",
      "Iterations 2900 loss_d 0.706101, loss_g 0.202227, lr 0.000050, time 1650.858183\n",
      "Iterations 3000 loss_d 0.679187, loss_g 0.199334, lr 0.000050, time 1707.470095\n",
      "Iterations 3100 loss_d 0.704375, loss_g 0.194234, lr 0.000050, time 1764.210614\n",
      "Iterations 3200 loss_d 0.644085, loss_g 0.191850, lr 0.000050, time 1821.027616\n",
      "Iterations 3300 loss_d 0.663145, loss_g 0.207307, lr 0.000050, time 1877.775921\n",
      "Iterations 3400 loss_d 0.649339, loss_g 0.216455, lr 0.000050, time 1934.542281\n",
      "Iterations 3500 loss_d 0.661348, loss_g 0.186366, lr 0.000050, time 1991.288287\n",
      "Iterations 3600 loss_d 0.591571, loss_g 0.214098, lr 0.000050, time 2048.037741\n",
      "Iterations 3700 loss_d 0.591225, loss_g 0.212677, lr 0.000050, time 2104.762814\n",
      "Iterations 3800 loss_d 0.597920, loss_g 0.200812, lr 0.000050, time 2161.402235\n",
      "Iterations 3900 loss_d 0.573709, loss_g 0.216813, lr 0.000050, time 2218.207773\n",
      "Iterations 4000 loss_d 0.578148, loss_g 0.195562, lr 0.000050, time 2275.009605\n",
      "Iterations 4100 loss_d 0.568524, loss_g 0.226203, lr 0.000050, time 2331.921704\n",
      "Iterations 4200 loss_d 0.567858, loss_g 0.202431, lr 0.000050, time 2388.748959\n",
      "Iterations 4300 loss_d 0.546142, loss_g 0.199942, lr 0.000050, time 2445.408473\n",
      "Iterations 4400 loss_d 0.571139, loss_g 0.198103, lr 0.000050, time 2502.145305\n",
      "Iterations 4500 loss_d 0.604858, loss_g 0.204158, lr 0.000050, time 2558.554719\n",
      "Iterations 4600 loss_d 0.584060, loss_g 0.221124, lr 0.000050, time 2615.354185\n",
      "Iterations 4700 loss_d 0.542891, loss_g 0.203118, lr 0.000050, time 2672.280644\n",
      "Iterations 4800 loss_d 0.563201, loss_g 0.215776, lr 0.000050, time 2729.197915\n",
      "Iterations 4900 loss_d 0.558621, loss_g 0.212517, lr 0.000050, time 2785.939190\n",
      "Iterations 5000 loss_d 0.539121, loss_g 0.204720, lr 0.000050, time 2842.731129\n",
      "Iterations 5100 loss_d 0.592590, loss_g 0.203517, lr 0.000050, time 2899.512800\n",
      "Iterations 5200 loss_d 0.539453, loss_g 0.205415, lr 0.000050, time 2956.318953\n",
      "Iterations 5300 loss_d 0.588477, loss_g 0.205517, lr 0.000050, time 3013.086605\n",
      "Iterations 5400 loss_d 0.586760, loss_g 0.209403, lr 0.000050, time 3069.929379\n",
      "Iterations 5500 loss_d 0.563992, loss_g 0.197643, lr 0.000050, time 3126.671099\n",
      "Iterations 5600 loss_d 0.562753, loss_g 0.214593, lr 0.000050, time 3183.413068\n",
      "Iterations 5700 loss_d 0.532524, loss_g 0.213045, lr 0.000050, time 3240.297879\n",
      "Iterations 5800 loss_d 0.576108, loss_g 0.210811, lr 0.000050, time 3297.065271\n",
      "Iterations 5900 loss_d 0.595705, loss_g 0.200448, lr 0.000050, time 3353.760414\n",
      "Iterations 6000 loss_d 0.630794, loss_g 0.204237, lr 0.000050, time 3410.349861\n",
      "Iterations 6100 loss_d 0.676755, loss_g 0.213709, lr 0.000050, time 3467.262717\n",
      "Iterations 6200 loss_d 0.703847, loss_g 0.197128, lr 0.000050, time 3523.951818\n",
      "Iterations 6300 loss_d 0.624159, loss_g 0.200362, lr 0.000050, time 3580.687721\n",
      "Iterations 6400 loss_d 0.666993, loss_g 0.212617, lr 0.000050, time 3637.302779\n",
      "Iterations 6500 loss_d 0.680377, loss_g 0.200664, lr 0.000050, time 3693.913328\n",
      "Iterations 6600 loss_d 0.660606, loss_g 0.190250, lr 0.000050, time 3750.667137\n",
      "Iterations 6700 loss_d 0.655619, loss_g 0.209651, lr 0.000050, time 3807.426404\n",
      "Iterations 6800 loss_d 0.679061, loss_g 0.205063, lr 0.000050, time 3864.187720\n",
      "Iterations 6900 loss_d 0.665117, loss_g 0.209959, lr 0.000050, time 3920.957629\n",
      "Iterations 7000 loss_d 0.645765, loss_g 0.212037, lr 0.000050, time 3977.455699\n",
      "Iterations 7100 loss_d 0.653292, loss_g 0.205402, lr 0.000050, time 4033.749981\n",
      "Iterations 7200 loss_d 0.616424, loss_g 0.209984, lr 0.000050, time 4090.487326\n",
      "Iterations 7300 loss_d 0.655245, loss_g 0.199885, lr 0.000050, time 4147.248141\n",
      "Iterations 7400 loss_d 0.616364, loss_g 0.210688, lr 0.000050, time 4204.008816\n",
      "Iterations 7500 loss_d 0.593847, loss_g 0.207542, lr 0.000050, time 4260.759222\n",
      "Iterations 7600 loss_d 0.599318, loss_g 0.207171, lr 0.000050, time 4317.519421\n",
      "Iterations 7700 loss_d 0.562579, loss_g 0.199266, lr 0.000050, time 4374.264842\n",
      "Iterations 7800 loss_d 0.573744, loss_g 0.209825, lr 0.000050, time 4431.007519\n",
      "Iterations 7900 loss_d 0.551006, loss_g 0.224376, lr 0.000050, time 4487.680892\n",
      "Iterations 8000 loss_d 0.532548, loss_g 0.210960, lr 0.000050, time 4544.295200\n",
      "Iterations 8100 loss_d 0.517421, loss_g 0.211630, lr 0.000050, time 4601.208859\n",
      "Iterations 8200 loss_d 0.484137, loss_g 0.203406, lr 0.000050, time 4658.028434\n",
      "Iterations 8300 loss_d 0.523025, loss_g 0.205353, lr 0.000050, time 4715.217875\n",
      "Iterations 8400 loss_d 0.479732, loss_g 0.203721, lr 0.000050, time 4772.433460\n",
      "Iterations 8500 loss_d 0.486597, loss_g 0.203225, lr 0.000050, time 4829.036263\n",
      "Iterations 8600 loss_d 0.501144, loss_g 0.206085, lr 0.000050, time 4885.666485\n",
      "Iterations 8700 loss_d 0.526152, loss_g 0.205688, lr 0.000050, time 4942.112618\n",
      "Iterations 8800 loss_d 0.535398, loss_g 0.210049, lr 0.000050, time 4998.723478\n",
      "Iterations 8900 loss_d 0.545961, loss_g 0.201366, lr 0.000050, time 5055.089750\n",
      "Iterations 9000 loss_d 0.615774, loss_g 0.216544, lr 0.000050, time 5111.619038\n",
      "Iterations 9100 loss_d 0.651197, loss_g 0.202571, lr 0.000050, time 5168.343414\n",
      "Iterations 9200 loss_d 0.665791, loss_g 0.210904, lr 0.000050, time 5224.813620\n",
      "Iterations 9300 loss_d 0.645608, loss_g 0.201752, lr 0.000050, time 5281.496826\n",
      "Iterations 9400 loss_d 0.654173, loss_g 0.203558, lr 0.000050, time 5338.186637\n",
      "Iterations 9500 loss_d 0.580236, loss_g 0.200163, lr 0.000050, time 5394.936941\n",
      "Iterations 9600 loss_d 0.587467, loss_g 0.199004, lr 0.000050, time 5451.638729\n",
      "Iterations 9700 loss_d 0.597851, loss_g 0.197997, lr 0.000050, time 5508.237472\n",
      "Iterations 9800 loss_d 0.584402, loss_g 0.195180, lr 0.000050, time 5564.901884\n",
      "Iterations 9900 loss_d 0.617284, loss_g 0.196739, lr 0.000050, time 5621.627094\n",
      "Iterations 10000 loss_d 0.504684, loss_g 0.216454, lr 0.000050, time 5678.276299\n",
      "Iterations 10100 loss_d 0.600701, loss_g 0.193597, lr 0.000050, time 5735.053336\n",
      "Iterations 10200 loss_d 0.503345, loss_g 0.202033, lr 0.000050, time 5791.769958\n",
      "Iterations 10300 loss_d 0.543927, loss_g 0.211614, lr 0.000050, time 5848.329268\n",
      "Iterations 10400 loss_d 0.527029, loss_g 0.200705, lr 0.000050, time 5905.154716\n",
      "Iterations 10500 loss_d 0.545194, loss_g 0.210623, lr 0.000050, time 5961.831640\n",
      "Iterations 10600 loss_d 0.547488, loss_g 0.197987, lr 0.000050, time 6018.629601\n",
      "Iterations 10700 loss_d 0.482194, loss_g 0.205406, lr 0.000050, time 6075.519434\n",
      "Iterations 10800 loss_d 0.522554, loss_g 0.200114, lr 0.000050, time 6132.380716\n",
      "Iterations 10900 loss_d 0.529874, loss_g 0.208433, lr 0.000050, time 6189.006156\n",
      "Iterations 11000 loss_d 0.495841, loss_g 0.192401, lr 0.000050, time 6245.843570\n",
      "Iterations 11100 loss_d 0.491884, loss_g 0.208834, lr 0.000050, time 6302.470860\n",
      "Iterations 11200 loss_d 0.530554, loss_g 0.208603, lr 0.000050, time 6359.032269\n",
      "Iterations 11300 loss_d 0.485135, loss_g 0.212188, lr 0.000050, time 6415.726223\n",
      "Iterations 11400 loss_d 0.514642, loss_g 0.210473, lr 0.000050, time 6472.181699\n",
      "Iterations 11500 loss_d 0.572232, loss_g 0.204735, lr 0.000050, time 6528.995760\n",
      "Iterations 11600 loss_d 0.553019, loss_g 0.214589, lr 0.000050, time 6585.790310\n",
      "Iterations 11700 loss_d 0.571258, loss_g 0.208826, lr 0.000050, time 6642.521093\n",
      "Iterations 11800 loss_d 0.674980, loss_g 0.221013, lr 0.000050, time 6699.324082\n",
      "Iterations 11900 loss_d 0.593207, loss_g 0.198845, lr 0.000050, time 6756.074069\n",
      "Iterations 12000 loss_d 0.620905, loss_g 0.215457, lr 0.000050, time 6812.839987\n",
      "Iterations 12100 loss_d 0.644614, loss_g 0.197703, lr 0.000050, time 6869.638398\n",
      "Iterations 12200 loss_d 0.626158, loss_g 0.193922, lr 0.000050, time 6926.372561\n",
      "Iterations 12300 loss_d 0.570695, loss_g 0.208382, lr 0.000050, time 6983.111543\n",
      "Iterations 12400 loss_d 0.621177, loss_g 0.207693, lr 0.000050, time 7039.832212\n",
      "Iterations 12500 loss_d 0.546761, loss_g 0.203883, lr 0.000050, time 7096.235011\n",
      "Iterations 12600 loss_d 0.637464, loss_g 0.193927, lr 0.000050, time 7152.990434\n",
      "Iterations 12700 loss_d 0.536750, loss_g 0.192398, lr 0.000050, time 7209.651672\n",
      "Iterations 12800 loss_d 0.562283, loss_g 0.200704, lr 0.000050, time 7266.316945\n",
      "Iterations 12900 loss_d 0.532353, loss_g 0.200825, lr 0.000050, time 7323.011492\n",
      "Iterations 13000 loss_d 0.576692, loss_g 0.211765, lr 0.000050, time 7379.649662\n",
      "Iterations 13100 loss_d 0.515125, loss_g 0.219690, lr 0.000050, time 7436.269625\n",
      "Iterations 13200 loss_d 0.536923, loss_g 0.207095, lr 0.000050, time 7492.689135\n",
      "Iterations 13300 loss_d 0.557766, loss_g 0.217193, lr 0.000050, time 7549.374452\n",
      "Iterations 13400 loss_d 0.510669, loss_g 0.198933, lr 0.000050, time 7606.043493\n",
      "Iterations 13500 loss_d 0.539779, loss_g 0.202795, lr 0.000050, time 7662.368389\n",
      "Iterations 13600 loss_d 0.537811, loss_g 0.203339, lr 0.000050, time 7718.682350\n",
      "Iterations 13700 loss_d 0.582563, loss_g 0.217049, lr 0.000050, time 7775.441671\n",
      "Iterations 13800 loss_d 0.576798, loss_g 0.206301, lr 0.000050, time 7831.838542\n",
      "Iterations 13900 loss_d 0.528606, loss_g 0.206067, lr 0.000050, time 7888.500630\n",
      "Iterations 14000 loss_d 0.556337, loss_g 0.222076, lr 0.000050, time 7945.207350\n",
      "Iterations 14100 loss_d 0.598597, loss_g 0.195241, lr 0.000050, time 8002.353370\n",
      "Iterations 14200 loss_d 0.491268, loss_g 0.198732, lr 0.000050, time 8059.232618\n",
      "Iterations 14300 loss_d 0.547195, loss_g 0.205974, lr 0.000050, time 8115.925804\n",
      "Iterations 14400 loss_d 0.540747, loss_g 0.206239, lr 0.000050, time 8172.809698\n",
      "Iterations 14500 loss_d 0.547694, loss_g 0.211669, lr 0.000050, time 8229.853317\n",
      "Iterations 14600 loss_d 0.521383, loss_g 0.200169, lr 0.000050, time 8286.701900\n",
      "Iterations 14700 loss_d 0.609328, loss_g 0.205013, lr 0.000050, time 8343.325707\n",
      "Iterations 14800 loss_d 0.490944, loss_g 0.212229, lr 0.000050, time 8400.027314\n",
      "Iterations 14900 loss_d 0.592449, loss_g 0.213721, lr 0.000050, time 8456.366935\n",
      "Iterations 15000 loss_d 0.584810, loss_g 0.206311, lr 0.000050, time 8512.919474\n",
      "Iterations 15100 loss_d 0.531088, loss_g 0.210848, lr 0.000050, time 8569.595706\n",
      "Iterations 15200 loss_d 0.533548, loss_g 0.207476, lr 0.000050, time 8626.369833\n",
      "Iterations 15300 loss_d 0.503129, loss_g 0.205946, lr 0.000050, time 8683.113523\n",
      "Iterations 15400 loss_d 0.492781, loss_g 0.212111, lr 0.000050, time 8739.484811\n",
      "Iterations 15500 loss_d 0.569379, loss_g 0.201812, lr 0.000050, time 8796.166470\n",
      "Iterations 15600 loss_d 0.484305, loss_g 0.214933, lr 0.000050, time 8852.858150\n",
      "Iterations 15700 loss_d 0.474683, loss_g 0.199718, lr 0.000050, time 8909.574325\n",
      "Iterations 15800 loss_d 0.512170, loss_g 0.205598, lr 0.000050, time 8966.205764\n",
      "Iterations 15900 loss_d 0.489619, loss_g 0.210514, lr 0.000050, time 9022.883579\n",
      "Iterations 16000 loss_d 0.503277, loss_g 0.217697, lr 0.000050, time 9079.549816\n",
      "Iterations 16100 loss_d 0.502160, loss_g 0.189667, lr 0.000050, time 9136.655089\n",
      "Iterations 16200 loss_d 0.456969, loss_g 0.214286, lr 0.000050, time 9193.494579\n",
      "Iterations 16300 loss_d 0.539727, loss_g 0.205349, lr 0.000050, time 9250.060070\n",
      "Iterations 16400 loss_d 0.497517, loss_g 0.216326, lr 0.000050, time 9306.689693\n",
      "Iterations 16500 loss_d 0.507248, loss_g 0.195918, lr 0.000050, time 9363.403770\n",
      "Iterations 16600 loss_d 0.467023, loss_g 0.209591, lr 0.000050, time 9419.999284\n",
      "Iterations 16700 loss_d 0.509345, loss_g 0.220247, lr 0.000050, time 9476.455649\n",
      "Iterations 16800 loss_d 0.510888, loss_g 0.217742, lr 0.000050, time 9533.008826\n",
      "Iterations 16900 loss_d 0.512729, loss_g 0.212381, lr 0.000050, time 9589.603366\n",
      "Iterations 17000 loss_d 0.510688, loss_g 0.205986, lr 0.000050, time 9645.997447\n",
      "Iterations 17100 loss_d 0.544709, loss_g 0.199763, lr 0.000050, time 9701.859280\n",
      "Iterations 17200 loss_d 0.534939, loss_g 0.200674, lr 0.000050, time 9758.485028\n",
      "Iterations 17300 loss_d 0.547601, loss_g 0.204277, lr 0.000050, time 9814.885449\n",
      "Iterations 17400 loss_d 0.555378, loss_g 0.216518, lr 0.000050, time 9871.010494\n",
      "Iterations 17500 loss_d 0.470930, loss_g 0.206606, lr 0.000050, time 9927.732382\n",
      "Iterations 17600 loss_d 0.495183, loss_g 0.202853, lr 0.000050, time 9984.360553\n",
      "Iterations 17700 loss_d 0.540509, loss_g 0.209246, lr 0.000050, time 10041.066956\n",
      "Iterations 17800 loss_d 0.445734, loss_g 0.220383, lr 0.000050, time 10097.735028\n",
      "Iterations 17900 loss_d 0.484112, loss_g 0.224055, lr 0.000050, time 10154.448678\n",
      "Iterations 18000 loss_d 0.440958, loss_g 0.208949, lr 0.000050, time 10211.193506\n",
      "Iterations 18100 loss_d 0.451263, loss_g 0.203327, lr 0.000050, time 10268.285052\n",
      "Iterations 18200 loss_d 0.450000, loss_g 0.203740, lr 0.000050, time 10325.021793\n",
      "Iterations 18300 loss_d 0.509721, loss_g 0.211429, lr 0.000050, time 10381.724619\n",
      "Iterations 18400 loss_d 0.440424, loss_g 0.214171, lr 0.000050, time 10438.510268\n",
      "Iterations 18500 loss_d 0.452231, loss_g 0.203919, lr 0.000050, time 10495.344054\n",
      "Iterations 18600 loss_d 0.514840, loss_g 0.213856, lr 0.000050, time 10552.127857\n",
      "Iterations 18700 loss_d 0.520703, loss_g 0.205983, lr 0.000050, time 10608.875415\n",
      "Iterations 18800 loss_d 0.489301, loss_g 0.200267, lr 0.000050, time 10665.550435\n",
      "Iterations 18900 loss_d 0.524734, loss_g 0.210809, lr 0.000050, time 10722.309768\n",
      "Iterations 19000 loss_d 0.491981, loss_g 0.218766, lr 0.000050, time 10779.005717\n",
      "Iterations 19100 loss_d 0.477865, loss_g 0.195429, lr 0.000050, time 10835.677673\n",
      "Iterations 19200 loss_d 0.463560, loss_g 0.197638, lr 0.000050, time 10892.102469\n",
      "Iterations 19300 loss_d 0.478216, loss_g 0.213140, lr 0.000050, time 10948.812869\n",
      "Iterations 19400 loss_d 0.431959, loss_g 0.202993, lr 0.000050, time 11005.925494\n",
      "Iterations 19500 loss_d 0.465899, loss_g 0.213417, lr 0.000050, time 11062.680092\n",
      "Iterations 19600 loss_d 0.473005, loss_g 0.210972, lr 0.000050, time 11119.353883\n",
      "Iterations 19700 loss_d 0.423053, loss_g 0.203121, lr 0.000050, time 11176.233988\n",
      "Iterations 19800 loss_d 0.459461, loss_g 0.212027, lr 0.000050, time 11233.169711\n",
      "Iterations 19900 loss_d 0.433930, loss_g 0.213683, lr 0.000050, time 11289.957186\n"
     ]
    }
   ],
   "source": [
    "util.train(dloader, \n",
    "           generator, \n",
    "           discriminator, \n",
    "           G_criterion, \n",
    "           D_criterion, \n",
    "           optimizerG, \n",
    "           optimizerD,\n",
    "           device=device,\n",
    "           batch = 16,\n",
    "           lr=5e-5,\n",
    "           lr_decay_steps = 10000,\n",
    "           lr_decay = 0.99,\n",
    "           L2_lambda = 0.001,\n",
    "           iterations = 20000,\n",
    "           display = 100,\n",
    "           model_name='default_setting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader = DataLoader.DataLoader(50, 25, './data/h3.6m/dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "walking\n",
      "\n",
      "milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |\n",
      "walking          | 0.331 | 0.436 | 0.585 | 0.635 | 0.648 | 0.676 |\n",
      "\n",
      "eating\n",
      "\n",
      "milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |\n",
      "eating           | 0.220 | 0.375 | 0.487 | 0.590 | 0.638 | 0.836 |\n",
      "\n",
      "smoking\n",
      "\n",
      "milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |\n",
      "smoking          | 0.302 | 0.466 | 0.816 | 0.735 | 0.701 | 1.048 |\n",
      "\n",
      "discussion\n",
      "\n",
      "milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |\n",
      "discussion       | 0.340 | 0.610 | 0.752 | 0.809 | 1.109 | 1.211 |\n",
      "\n",
      "directions\n",
      "\n",
      "milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |\n",
      "directions       | 0.425 | 0.591 | 0.681 | 0.720 | 0.747 | 0.939 |\n",
      "\n",
      "greeting\n",
      "\n",
      "milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |\n",
      "greeting         | 0.518 | 0.755 | 1.028 | 1.136 | 1.242 | 1.122 |\n",
      "\n",
      "phoning\n",
      "\n",
      "milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |\n",
      "phoning          | 0.586 | 1.101 | 1.368 | 1.450 | 1.169 | 1.129 |\n",
      "\n",
      "posing\n",
      "\n",
      "milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |\n",
      "posing           | 0.392 | 0.703 | 1.133 | 1.241 | 1.463 | 1.551 |\n",
      "\n",
      "purchases\n",
      "\n",
      "milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |\n",
      "purchases        | 0.586 | 0.762 | 0.921 | 0.934 | 1.247 | 1.740 |\n",
      "\n",
      "sitting\n",
      "\n",
      "milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |\n",
      "sitting          | 0.438 | 0.594 | 0.856 | 0.970 | 0.943 | 1.217 |\n",
      "\n",
      "sittingdown\n",
      "\n",
      "milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |\n",
      "sittingdown      | 0.474 | 0.721 | 0.951 | 0.982 | 1.012 | 1.139 |\n",
      "\n",
      "takingphoto\n",
      "\n",
      "milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |\n",
      "takingphoto      | 0.281 | 0.497 | 0.675 | 0.746 | 0.798 | 0.780 |\n",
      "\n",
      "waiting\n",
      "\n",
      "milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |\n",
      "waiting          | 0.373 | 0.600 | 0.943 | 1.071 | 1.250 | 1.406 |\n",
      "\n",
      "walkingdog\n",
      "\n",
      "milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |\n",
      "walkingdog       | 0.601 | 0.863 | 1.035 | 1.094 | 1.191 | 1.147 |\n",
      "\n",
      "walkingtogether\n",
      "\n",
      "milliseconds     |    80 |   160 |   320 |   400 |   560 |  1000 |\n",
      "walkingtogether  | 0.332 | 0.507 | 0.603 | 0.584 | 0.636 | 0.976 |\n",
      "0.08219700666959398\n"
     ]
    }
   ],
   "source": [
    "util.InferenceSample(dloader, generator, model_name='default_setting', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grave Yard **********************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/20000]\tLoss_D: 1.3884\tLoss_G: 0.2619\n",
      "[100/20000]\tLoss_D: 1.2983\tLoss_G: 0.2432\n",
      "[200/20000]\tLoss_D: 1.1957\tLoss_G: 0.2439\n",
      "[300/20000]\tLoss_D: 0.9976\tLoss_G: 0.3253\n",
      "[400/20000]\tLoss_D: 1.1649\tLoss_G: 0.1919\n",
      "[500/20000]\tLoss_D: 1.1214\tLoss_G: 0.2258\n",
      "[600/20000]\tLoss_D: 1.0522\tLoss_G: 0.3107\n",
      "[700/20000]\tLoss_D: 0.9603\tLoss_G: 0.2668\n",
      "[800/20000]\tLoss_D: 1.2217\tLoss_G: 0.2764\n",
      "[900/20000]\tLoss_D: 0.9751\tLoss_G: 0.4168\n",
      "[1000/20000]\tLoss_D: 0.9488\tLoss_G: 0.2173\n",
      "[1100/20000]\tLoss_D: 1.0963\tLoss_G: 0.2862\n",
      "[1200/20000]\tLoss_D: 0.8042\tLoss_G: 0.1657\n",
      "[1300/20000]\tLoss_D: 0.7598\tLoss_G: 0.2153\n",
      "[1400/20000]\tLoss_D: 0.7531\tLoss_G: 0.3976\n",
      "[1500/20000]\tLoss_D: 1.0152\tLoss_G: 0.2257\n",
      "[1600/20000]\tLoss_D: 0.7739\tLoss_G: 0.2131\n",
      "[1700/20000]\tLoss_D: 0.7638\tLoss_G: 0.2901\n",
      "[1800/20000]\tLoss_D: 0.9798\tLoss_G: 0.1239\n",
      "[1900/20000]\tLoss_D: 1.3205\tLoss_G: 0.1534\n",
      "[2000/20000]\tLoss_D: 0.7119\tLoss_G: 0.1554\n",
      "[2100/20000]\tLoss_D: 0.9848\tLoss_G: 0.1804\n",
      "[2200/20000]\tLoss_D: 0.8662\tLoss_G: 0.1224\n",
      "[2300/20000]\tLoss_D: 0.8907\tLoss_G: 0.2597\n",
      "[2400/20000]\tLoss_D: 0.7356\tLoss_G: 0.4877\n",
      "[2500/20000]\tLoss_D: 0.6413\tLoss_G: 0.1978\n",
      "[2600/20000]\tLoss_D: 0.6767\tLoss_G: 0.1662\n",
      "[2700/20000]\tLoss_D: 0.7201\tLoss_G: 0.2242\n",
      "[2800/20000]\tLoss_D: 0.9045\tLoss_G: 0.2158\n",
      "[2900/20000]\tLoss_D: 0.4212\tLoss_G: 0.1940\n",
      "[3000/20000]\tLoss_D: 0.6729\tLoss_G: 0.2407\n",
      "[3100/20000]\tLoss_D: 0.5695\tLoss_G: 0.2472\n",
      "[3200/20000]\tLoss_D: 0.6402\tLoss_G: 0.1935\n",
      "[3300/20000]\tLoss_D: 0.4455\tLoss_G: 0.1270\n",
      "[3400/20000]\tLoss_D: 0.5060\tLoss_G: 0.1897\n",
      "[3500/20000]\tLoss_D: 0.2833\tLoss_G: 0.1776\n",
      "[3600/20000]\tLoss_D: 0.7702\tLoss_G: 0.1741\n",
      "[3700/20000]\tLoss_D: 0.7038\tLoss_G: 0.1775\n",
      "[3800/20000]\tLoss_D: 0.3031\tLoss_G: 0.1931\n",
      "[3900/20000]\tLoss_D: 0.3971\tLoss_G: 0.1804\n",
      "[4000/20000]\tLoss_D: 0.2261\tLoss_G: 0.1663\n",
      "[4100/20000]\tLoss_D: 0.4453\tLoss_G: 0.2165\n",
      "[4200/20000]\tLoss_D: 0.6589\tLoss_G: 0.2011\n",
      "[4300/20000]\tLoss_D: 0.4446\tLoss_G: 0.1972\n",
      "[4400/20000]\tLoss_D: 0.5358\tLoss_G: 0.2199\n",
      "[4500/20000]\tLoss_D: 0.4048\tLoss_G: 0.2358\n",
      "[4600/20000]\tLoss_D: 0.1781\tLoss_G: 0.1966\n",
      "[4700/20000]\tLoss_D: 0.7668\tLoss_G: 0.1618\n",
      "[4800/20000]\tLoss_D: 0.3292\tLoss_G: 0.1827\n",
      "[4900/20000]\tLoss_D: 0.0857\tLoss_G: 0.2087\n",
      "[5000/20000]\tLoss_D: 0.2606\tLoss_G: 0.1759\n",
      "[5100/20000]\tLoss_D: 0.3248\tLoss_G: 0.2037\n",
      "[5200/20000]\tLoss_D: 0.7323\tLoss_G: 0.1335\n",
      "[5300/20000]\tLoss_D: 0.6089\tLoss_G: 0.1706\n",
      "[5400/20000]\tLoss_D: 0.6274\tLoss_G: 0.1961\n",
      "[5500/20000]\tLoss_D: 0.5661\tLoss_G: 0.2609\n",
      "[5600/20000]\tLoss_D: 0.7847\tLoss_G: 0.1951\n",
      "[5700/20000]\tLoss_D: 0.3439\tLoss_G: 0.2730\n",
      "[5800/20000]\tLoss_D: 0.4881\tLoss_G: 0.1953\n",
      "[5900/20000]\tLoss_D: 0.5113\tLoss_G: 0.2535\n",
      "[6000/20000]\tLoss_D: 0.6621\tLoss_G: 0.2091\n",
      "[6100/20000]\tLoss_D: 0.4651\tLoss_G: 0.1589\n",
      "[6200/20000]\tLoss_D: 0.7395\tLoss_G: 0.1853\n",
      "[6300/20000]\tLoss_D: 0.7509\tLoss_G: 0.1712\n",
      "[6400/20000]\tLoss_D: 0.6293\tLoss_G: 0.1602\n",
      "[6500/20000]\tLoss_D: 1.1502\tLoss_G: 0.2909\n",
      "[6600/20000]\tLoss_D: 0.6367\tLoss_G: 0.1830\n",
      "[6700/20000]\tLoss_D: 1.0060\tLoss_G: 0.2530\n",
      "[6800/20000]\tLoss_D: 0.8099\tLoss_G: 0.1428\n",
      "[6900/20000]\tLoss_D: 0.7129\tLoss_G: 0.1670\n",
      "[7000/20000]\tLoss_D: 0.6875\tLoss_G: 0.1874\n",
      "[7100/20000]\tLoss_D: 0.7379\tLoss_G: 0.1798\n",
      "[7200/20000]\tLoss_D: 0.7326\tLoss_G: 0.1701\n",
      "[7300/20000]\tLoss_D: 0.4667\tLoss_G: 0.3015\n",
      "[7400/20000]\tLoss_D: 0.3177\tLoss_G: 0.2179\n",
      "[7500/20000]\tLoss_D: 0.8852\tLoss_G: 0.2142\n",
      "[7600/20000]\tLoss_D: 0.5566\tLoss_G: 0.2989\n",
      "[7700/20000]\tLoss_D: 0.9179\tLoss_G: 0.1774\n",
      "[7800/20000]\tLoss_D: 0.2809\tLoss_G: 0.1731\n",
      "[7900/20000]\tLoss_D: 0.6618\tLoss_G: 0.1632\n",
      "[8000/20000]\tLoss_D: 0.4107\tLoss_G: 0.2263\n",
      "[8100/20000]\tLoss_D: 0.7878\tLoss_G: 0.1471\n",
      "[8200/20000]\tLoss_D: 0.4733\tLoss_G: 0.2452\n",
      "[8300/20000]\tLoss_D: 0.8246\tLoss_G: 0.3202\n",
      "[8400/20000]\tLoss_D: 0.8795\tLoss_G: 0.2336\n",
      "[8500/20000]\tLoss_D: 0.8001\tLoss_G: 0.2293\n",
      "[8600/20000]\tLoss_D: 0.5858\tLoss_G: 0.1777\n",
      "[8700/20000]\tLoss_D: 0.4084\tLoss_G: 0.1799\n",
      "[8800/20000]\tLoss_D: 0.6848\tLoss_G: 0.3694\n",
      "[8900/20000]\tLoss_D: 0.5314\tLoss_G: 0.1521\n",
      "[9000/20000]\tLoss_D: 0.7267\tLoss_G: 0.2040\n",
      "[9100/20000]\tLoss_D: 0.5138\tLoss_G: 0.1775\n",
      "[9200/20000]\tLoss_D: 0.5019\tLoss_G: 0.1712\n",
      "[9300/20000]\tLoss_D: 0.6042\tLoss_G: 0.1907\n",
      "[9400/20000]\tLoss_D: 0.6492\tLoss_G: 0.2219\n",
      "[9500/20000]\tLoss_D: 0.3749\tLoss_G: 0.4263\n",
      "[9600/20000]\tLoss_D: 0.6791\tLoss_G: 0.2291\n",
      "[9700/20000]\tLoss_D: 0.4511\tLoss_G: 0.1478\n",
      "[9800/20000]\tLoss_D: 0.7179\tLoss_G: 0.1833\n",
      "[9900/20000]\tLoss_D: 0.8549\tLoss_G: 0.2791\n",
      "[10000/20000]\tLoss_D: 0.3582\tLoss_G: 0.2012\n",
      "[10100/20000]\tLoss_D: 0.7120\tLoss_G: 0.1741\n",
      "[10200/20000]\tLoss_D: 0.6698\tLoss_G: 0.2205\n",
      "[10300/20000]\tLoss_D: 0.3139\tLoss_G: 0.1336\n",
      "[10400/20000]\tLoss_D: 0.2935\tLoss_G: 0.2145\n",
      "[10500/20000]\tLoss_D: 0.8468\tLoss_G: 0.5260\n",
      "[10600/20000]\tLoss_D: 0.8475\tLoss_G: 0.1201\n",
      "[10700/20000]\tLoss_D: 0.8889\tLoss_G: 0.1672\n",
      "[10800/20000]\tLoss_D: 0.2467\tLoss_G: 0.2623\n",
      "[10900/20000]\tLoss_D: 0.3360\tLoss_G: 0.3026\n",
      "[11000/20000]\tLoss_D: 0.5200\tLoss_G: 0.1474\n",
      "[11100/20000]\tLoss_D: 0.5516\tLoss_G: 0.2448\n",
      "[11200/20000]\tLoss_D: 0.4581\tLoss_G: 0.4134\n",
      "[11300/20000]\tLoss_D: 0.7758\tLoss_G: 0.2753\n",
      "[11400/20000]\tLoss_D: 0.7758\tLoss_G: 0.1793\n",
      "[11500/20000]\tLoss_D: 0.5297\tLoss_G: 0.2242\n",
      "[11600/20000]\tLoss_D: 0.5765\tLoss_G: 0.2351\n",
      "[11700/20000]\tLoss_D: 0.2823\tLoss_G: 0.1690\n",
      "[11800/20000]\tLoss_D: 0.6411\tLoss_G: 0.1572\n",
      "[11900/20000]\tLoss_D: 0.7635\tLoss_G: 0.1690\n",
      "[12000/20000]\tLoss_D: 0.7455\tLoss_G: 0.2126\n",
      "[12100/20000]\tLoss_D: 0.2379\tLoss_G: 0.2308\n",
      "[12200/20000]\tLoss_D: 0.4185\tLoss_G: 0.4386\n",
      "[12300/20000]\tLoss_D: 0.4481\tLoss_G: 0.1765\n",
      "[12400/20000]\tLoss_D: 0.8202\tLoss_G: 0.2253\n",
      "[12500/20000]\tLoss_D: 0.3268\tLoss_G: 0.1520\n",
      "[12600/20000]\tLoss_D: 0.2241\tLoss_G: 0.2271\n",
      "[12700/20000]\tLoss_D: 0.5845\tLoss_G: 0.1880\n",
      "[12800/20000]\tLoss_D: 0.3843\tLoss_G: 0.2888\n",
      "[12900/20000]\tLoss_D: 1.2988\tLoss_G: 0.1928\n",
      "[13000/20000]\tLoss_D: 0.2519\tLoss_G: 0.1316\n",
      "[13100/20000]\tLoss_D: 0.4727\tLoss_G: 0.1865\n",
      "[13200/20000]\tLoss_D: 0.5836\tLoss_G: 0.1909\n",
      "[13300/20000]\tLoss_D: 0.5776\tLoss_G: 0.1522\n",
      "[13400/20000]\tLoss_D: 0.3677\tLoss_G: 0.1828\n",
      "[13500/20000]\tLoss_D: 0.3911\tLoss_G: 0.2086\n",
      "[13600/20000]\tLoss_D: 0.6901\tLoss_G: 0.3300\n",
      "[13700/20000]\tLoss_D: 0.7176\tLoss_G: 0.1965\n",
      "[13800/20000]\tLoss_D: 0.4278\tLoss_G: 0.3013\n",
      "[13900/20000]\tLoss_D: 0.1924\tLoss_G: 0.1818\n",
      "[14000/20000]\tLoss_D: 0.5846\tLoss_G: 0.1720\n",
      "[14100/20000]\tLoss_D: 0.4322\tLoss_G: 0.3403\n",
      "[14200/20000]\tLoss_D: 0.1320\tLoss_G: 0.1400\n",
      "[14300/20000]\tLoss_D: 0.4182\tLoss_G: 0.1861\n",
      "[14400/20000]\tLoss_D: 0.5954\tLoss_G: 0.2367\n",
      "[14500/20000]\tLoss_D: 0.5261\tLoss_G: 0.2111\n",
      "[14600/20000]\tLoss_D: 0.4585\tLoss_G: 0.1537\n",
      "[14700/20000]\tLoss_D: 0.2664\tLoss_G: 0.2262\n",
      "[14800/20000]\tLoss_D: 0.3571\tLoss_G: 0.1478\n",
      "[14900/20000]\tLoss_D: 0.7226\tLoss_G: 0.1739\n",
      "[15000/20000]\tLoss_D: 0.3056\tLoss_G: 0.3400\n",
      "[15100/20000]\tLoss_D: 0.5582\tLoss_G: 0.1610\n",
      "[15200/20000]\tLoss_D: 0.6225\tLoss_G: 0.1489\n",
      "[15300/20000]\tLoss_D: 0.8327\tLoss_G: 0.2009\n",
      "[15400/20000]\tLoss_D: 0.5047\tLoss_G: 0.2411\n",
      "[15500/20000]\tLoss_D: 0.4417\tLoss_G: 0.1862\n",
      "[15600/20000]\tLoss_D: 0.7970\tLoss_G: 0.1574\n",
      "[15700/20000]\tLoss_D: 0.5067\tLoss_G: 0.2054\n",
      "[15800/20000]\tLoss_D: 0.6409\tLoss_G: 0.1613\n",
      "[15900/20000]\tLoss_D: 0.4428\tLoss_G: 0.1516\n",
      "[16000/20000]\tLoss_D: 0.6296\tLoss_G: 0.1793\n",
      "[16100/20000]\tLoss_D: 0.2990\tLoss_G: 0.2022\n",
      "[16200/20000]\tLoss_D: 0.3453\tLoss_G: 0.1894\n",
      "[16300/20000]\tLoss_D: 0.1452\tLoss_G: 0.1859\n",
      "[16400/20000]\tLoss_D: 0.7893\tLoss_G: 0.2703\n",
      "[16500/20000]\tLoss_D: 0.3491\tLoss_G: 0.2103\n",
      "[16600/20000]\tLoss_D: 0.2286\tLoss_G: 0.2209\n",
      "[16700/20000]\tLoss_D: 0.3325\tLoss_G: 0.1885\n",
      "[16800/20000]\tLoss_D: 0.5284\tLoss_G: 0.1703\n",
      "[16900/20000]\tLoss_D: 0.3210\tLoss_G: 0.1307\n",
      "[17000/20000]\tLoss_D: 0.5309\tLoss_G: 0.1967\n",
      "[17100/20000]\tLoss_D: 0.6819\tLoss_G: 0.2124\n",
      "[17200/20000]\tLoss_D: 0.4504\tLoss_G: 0.1777\n",
      "[17300/20000]\tLoss_D: 0.5652\tLoss_G: 0.2162\n",
      "[17400/20000]\tLoss_D: 0.3450\tLoss_G: 0.2034\n",
      "[17500/20000]\tLoss_D: 0.3441\tLoss_G: 0.2909\n",
      "[17600/20000]\tLoss_D: 0.8142\tLoss_G: 0.2947\n",
      "[17700/20000]\tLoss_D: 0.3967\tLoss_G: 0.1990\n",
      "[17800/20000]\tLoss_D: 0.4263\tLoss_G: 0.1735\n",
      "[17900/20000]\tLoss_D: 0.5879\tLoss_G: 0.1985\n",
      "[18000/20000]\tLoss_D: 0.7155\tLoss_G: 0.2150\n",
      "[18100/20000]\tLoss_D: 0.5360\tLoss_G: 0.2166\n",
      "[18200/20000]\tLoss_D: 0.5152\tLoss_G: 0.2373\n",
      "[18300/20000]\tLoss_D: 0.5646\tLoss_G: 0.2656\n",
      "[18400/20000]\tLoss_D: 0.4636\tLoss_G: 0.2218\n",
      "[18500/20000]\tLoss_D: 0.2926\tLoss_G: 0.2179\n",
      "[18600/20000]\tLoss_D: 0.4626\tLoss_G: 0.1814\n",
      "[18700/20000]\tLoss_D: 0.5807\tLoss_G: 0.2177\n",
      "[18800/20000]\tLoss_D: 0.7800\tLoss_G: 0.2104\n",
      "[18900/20000]\tLoss_D: 0.6150\tLoss_G: 0.2231\n",
      "[19000/20000]\tLoss_D: 0.2542\tLoss_G: 0.2169\n",
      "[19100/20000]\tLoss_D: 0.5252\tLoss_G: 0.1175\n",
      "[19200/20000]\tLoss_D: 0.1686\tLoss_G: 0.1858\n",
      "[19300/20000]\tLoss_D: 0.2321\tLoss_G: 0.2356\n",
      "[19400/20000]\tLoss_D: 0.7282\tLoss_G: 0.1960\n",
      "[19500/20000]\tLoss_D: 0.4445\tLoss_G: 0.1643\n",
      "[19600/20000]\tLoss_D: 0.7797\tLoss_G: 0.2735\n",
      "[19700/20000]\tLoss_D: 0.4557\tLoss_G: 0.1773\n",
      "[19800/20000]\tLoss_D: 0.1438\tLoss_G: 0.1689\n",
      "[19900/20000]\tLoss_D: 0.5037\tLoss_G: 0.1559\n"
     ]
    }
   ],
   "source": [
    "iterations = 20000\n",
    "for i in range(iterations):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    # print('Memory usage when training begins: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "    #                                                torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    \n",
    "    # print(\"****************train discriminator*****************\")\n",
    "    encoder_data, discriminator_data, yhat = dloader.get_train_batch(16)\n",
    "\n",
    "    encoder_data = torch.from_numpy(encoder_data).float().to(device)\n",
    "    # print(\"encoder_data size: {} bytes => {} GB\".format(encoder_data.element_size() * encoder_data.nelement(),\n",
    "    #                                                     encoder_data.element_size() * encoder_data.nelement()/1e9))\n",
    "    \n",
    "    discriminator_data = torch.from_numpy(discriminator_data).float().to(device)\n",
    "    # print(\"discriminator_data size: {} bytes => {} GB\".format(discriminator_data.element_size() * discriminator_data.nelement(),\n",
    "    #                                                           discriminator_data.element_size() * discriminator_data.nelement()/1e9))\n",
    "    \n",
    "    yhat = torch.from_numpy(yhat).float().to(device)\n",
    "    # print(\"yhat size: {} bytes => {} GB\".format(yhat.element_size() * yhat.nelement(),\n",
    "    #                                             yhat.element_size() * yhat.nelement()/1e9))\n",
    "    \n",
    "    # expected_seq = discriminator_data[:, 50:, :, :]\n",
    "    # print(\"expected_seq size: {} bytes => {} GB\".format(expected_seq.element_size() * expected_seq.nelement(),\n",
    "    #                                                     expected_seq.element_size() * expected_seq.nelement()/1e9))\n",
    "    for param in discriminator.parameters():\n",
    "        param.grad = None\n",
    "    # optimizerD.zero_grad()\n",
    "    # print('Memory usage after discriminator zero_grad: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "    #                                                torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    # with torch.cuda.amp.autocast():\n",
    "    d_logits_real = discriminator.forward(discriminator_data, yhat)\n",
    "    # print('Memory usage after d_logits_real forward: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "    #                                                torch.cuda.memory_allocated(device=device)/1e9))\n",
    "\n",
    "    d_loss_real = D_criterion(d_logits_real, torch.ones_like(d_logits_real))\n",
    "    # print('Memory usage after d_loss_real: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "    #                                                torch.cuda.memory_allocated(device=device)/1e9))\n",
    "\n",
    "    d_loss_real.backward()\n",
    "    # print('Memory usage after d_loss_real backward: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "    #                                                torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    # with torch.cuda.amp.autocast():\n",
    "    predicted_seq, predicted_action, generated_sample = generator.forward(encoder_data, discriminator_data)\n",
    "    \n",
    "    # print('Memory usage after generator forward: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "    #                                                torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    d_logits_fake = discriminator.forward(generated_sample, yhat)\n",
    "    # print('Memory usage after d_logits_fake forward: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "    #                                                torch.cuda.memory_allocated(device=device)/1e9))\n",
    "\n",
    "    d_loss_fake = D_criterion(d_logits_fake, torch.zeros_like(d_logits_fake))\n",
    "    # print('Memory usage after d_loss_fake : {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "    #                                                torch.cuda.memory_allocated(device=device)/1e9))\n",
    "\n",
    "    d_loss_fake.backward()\n",
    "    # print('Memory usage after d_loss_fake backward: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "    #                                                torch.cuda.memory_allocated(device=device)/1e9))\n",
    "\n",
    "    loss_discriminator = d_loss_real + d_loss_fake\n",
    "    # print('Memory usage after loss_discriminator summed: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "    #                                                torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    optimizerD.step()\n",
    "    # print('Memory usage after optimizerD stepped: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "    #                                                torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    \n",
    "    # print(\"****************train generator*****************\")\n",
    "    encoder_data, discriminator_data, yhat = dloader.get_train_batch(16)\n",
    "\n",
    "    encoder_data = torch.from_numpy(encoder_data).float().to(device)\n",
    "    # print(\"encoder_data size: {} bytes => {} GB\".format(encoder_data.element_size() * encoder_data.nelement(),\n",
    "    #                                                     encoder_data.element_size() * encoder_data.nelement()/1e9))\n",
    "    \n",
    "    discriminator_data = torch.from_numpy(discriminator_data).float().to(device)\n",
    "    # print(\"discriminator_data size: {} bytes => {} GB\".format(discriminator_data.element_size() * discriminator_data.nelement(),\n",
    "    #                                                           discriminator_data.element_size() * discriminator_data.nelement()/1e9))\n",
    "    \n",
    "    yhat = torch.from_numpy(yhat).float().to(device)\n",
    "    # print(\"yhat size: {} bytes => {} GB\".format(yhat.element_size() * yhat.nelement(),\n",
    "    #                                             yhat.element_size() * yhat.nelement()/1e9))\n",
    "    \n",
    "    expected_seq = discriminator_data[:, 50:, :, :]\n",
    "    # print(\"expected_seq size: {} bytes => {} GB\".format(expected_seq.element_size() * expected_seq.nelement(),\n",
    "    #                                                     expected_seq.element_size() * expected_seq.nelement()/1e9))\n",
    "    \n",
    "    for param in generator.parameters():\n",
    "        param.grad = None\n",
    "    # optimizerG.zero_grad()\n",
    "    # with torch.cuda.amp.autocast():\n",
    "    predicted_seq, predicted_action, generated_sample = generator.forward(encoder_data, discriminator_data)\n",
    "    # print('Memory usage after generator forward: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "    #                                                torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    ReconstructError = G_criterion(predicted_seq, expected_seq)\n",
    "    # print('Memory usage after ReconstructError : {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "    #                                                torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    ReconstructError.backward(retain_graph=True)\n",
    "    # print('Memory usage after ReconstructError backward: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "    #                                                torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    # with torch.cuda.amp.autocast():\n",
    "    d_logits_fake = discriminator.forward(generated_sample, yhat)\n",
    "    # print('Memory usage after discriminator foward : {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "    #                                                torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    g_loss = D_criterion(d_logits_fake, torch.ones_like(d_logits_fake)) * torch.tensor(0.01)\n",
    "    # print('Memory usage after g_loss : {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "    #                                                torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    g_loss.backward()\n",
    "    # print('Memory usage after g_loss backward: {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "    #                                                torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    loss_generator = ReconstructError + g_loss\n",
    "    # print('Memory usage after loss_generator summed : {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "    #                                                torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    \n",
    "    optimizerG.step()\n",
    "    # print('Memory usage after optimizerG stepped : {} bytes => {} GB'.format(torch.cuda.memory_allocated(device=device), \n",
    "    #                                                torch.cuda.memory_allocated(device=device)/1e9))\n",
    "    \n",
    "    # Output training stats\n",
    "    if i % 100 == 0:\n",
    "        print('[%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f'\n",
    "            % (i, iterations, loss_discriminator.item(), loss_generator.item()))\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(), 'save_dir/generator.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(discriminator.state_dict(), 'save_dir/discriminator.pt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e9b5e93101e5958f9cd028a9f43ed128e885f27a05c22831a3d71acc3bca11d"
  },
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
